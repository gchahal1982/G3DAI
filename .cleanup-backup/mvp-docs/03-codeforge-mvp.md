# aura – Next‑Generation AI‑Assisted Development Platform  
**Version 3 – 18 July 2025**

---

## Executive Summary
|           | Detail |
|-----------|--------|
| **Service** | aura — an end‑to‑end AI development workspace that couples local privacy‑first coding assistance with cloud super‑context reasoning and G3D‑powered 3‑D visualisation. |
| **MVP Target** | A production‑ready hybrid IDE/CLI plug‑in (VS Code + NeoVim) and standalone desktop client, shipping with offline quantised models and optional secure cloud burst. |
| **Revenue Potential** | \$40‑120 M ARR within 3 years. |
| **Investment Required** | \$3.0 M over 9 months (32 devs inc. 8 G3D specialists). |
| **Strategic Goal** | Become the de‑facto "AI forge" that transforms the way teams plan, write, visualise and ship code. |

---

## 1 | Purpose & Vision
aura's mission is to **compress idea‑to‑deployment time from days to minutes** by fusing:

* **AI Swarms** — orchestrated specialist agents for design, code, test, security & docs.  
* **Immersive 3‑D Visualisation** — G3D renders of architecture, call‑graphs & live intent graphs.  
* **Hybrid AI** — lightning‑fast local models for daily flow, super‑context cloud brains for deep analysis.  
* **Enterprise‑grade Trust** — zero‑trust local inference, SOC 2 cloud, SBOM & signed artefacts.

---

## 2 | Strategic Insights
* **Developers want privacy & speed** → local LLMs are exploding in adoption.  
* **Context windows still pinch** → burst‑to‑cloud for 70 k‑128 k tokens solves repo‑scale reasoning.  
* **Tools ≠ UX** → first‑wave AI plug‑ins look bolted‑on; devs crave *clarity* of what the agent does.  
* **Visual cognition beats text** → codebase comprehension jumps when structure is spatially rendered.  
* **Open ecosystem wins** → extensible marketplaces and OSS‑friendly licences encourage viral growth.

---

## 3 | Market Opportunity
| Scope | Value |
|-------|-------|
| **TAM** (Dev tools) | \$26.5 B |
| **SAM** (AI‑powered dev) | \$8.2 B |
| **SOM** (Hybrid + G3D niche) | \$1.2 B |

### Target Customers & ACVs
1. **Enterprise Teams** — \$100 k‑1 M / yr  
2. **SaaS/Product Companies** — \$25 k‑500 k / yr  
3. **DevOps & Platform Squads** — \$50 k‑300 k / yr  
4. **Consultancies & SIs** — \$30 k‑200 k / yr  
5. **Gov & Defence** — \$75 k‑750 k / yr

## 3.1 User Personas & JTBD

| Persona | Pain Point | "Job‑to‑Be‑Done" | Key Success Metric |
|---------|------------|-----------------|--------------------|
| **Indie Hacker** | Context‑switch fatigue, infra cost | "Ship a side‑project tonight without installing Docker." | Prototype deployed in < 4 h |
| **Senior Backend Engineer** | Legacy refactor risk | "Confidently migrate a monolith to services without breaking prod." | < 1 % rollback rate |
| **Team Lead / EM** | Review bottlenecks | "Keep PR queue < 24 h while mentoring juniors." | Review SLA met 95 % |
| **Security Engineer** | Unknown code paths | "Surface Oss‑vulnerable deps & secret leaks continuously." | Zero critical CVEs at release |
| **DevRel Advocate** | Community engagement | "Author demos & tutorials that wow in < 1 day." | GitHub stars & Discord joins |

### 3.2 Core Workflow Scenarios

1. **Greenfield Wizard** – "Generate a Rust microservice template with CI, tests, Terraform."  
2. **Surgical Bug Fix** – Point AI at failing test; agent proposes patch, auto‑opens ghost‑branch PR.  
3. **Architectural Refactor** – Visual Intent‑Graph highlights coupling; AI splits modules, updates Docs.  
4. **Security Sweep** – Security‑Swarm agent runs SAST+SBOM, inserts fixes & compliance badges.  
5. **XR Code Walkthrough** – VR session: lead walks new hire through 3‑D call‑graph, AI answers Q&A.

---

## 4 | Competitive Analysis

| Competitor | Differentiator | Model Strat. | Collab | Visual‑isation | Arch/Plan | UX Mode | Extens. | Enterprise | Security | Licence |
|------------|----------------|--------------|--------|----------------|-----------|---------|---------|------------|----------|---------|
| **Cursor** | AI‑native editor | Cloud | Basic | Limited | Medium | Custom IDE | Mod. | Growing | Mod. | Proprietary |
| **GitHub Copilot** | Inline completion | Cloud | Basic | None | Low | IDE plug‑in | Low | Strong via GitHub | Mod. | Proprietary |
| **Windsurf** | "Vibe‑coding" web IDE | Cloud | Mod. | Basic | Low | Web IDE | Mod. | Strong | Mod. | Proprietary |
| **Lovable** | NL→App builder | Cloud | Good | Low | Medium | Chat/web | Low | Growing | Mod. | Proprietary |
| **Replit** | Browser IDE + agent | Cloud | Excellent | Low | Medium | Web IDE | Mod. | Mod. | Mod. | Proprietary |
| **Claude Code** | CLI agentic | Hybrid | Low | None | High | CLI | High | Mod. | Strong | Proprietary |
| **Cline** | OSS plan/act agent | Local | Mod. | None | Medium | VS Code ext | High | Low | Good | OSS |
| **Roo** | Multi‑agent OSS | Local | Good | Med. | High | VS Code ext | High | Low | Good | OSS |
| **Kilo Code** | Modular OSS | Local | Excellent | Med. | High | VS Code ext | Very High | Mod. | Excellent | OSS |
| **aura** | **3‑D G3D viz + AI swarms + hybrid local/cloud** | **Hybrid** | **Excellent** | **Adv. 3‑D** | **Intent‑Graph** | IDE + Desktop | **Marketplace** | **Enterprise suite** | **Zero‑trust** | **Hybrid (commercial core + open SDK)** |

**Why aura Wins**

* **Visual Clarity** – Only platform with *real‑time 3‑D* architecture & dependency renderings.  
* **Hybrid Speed + Power** – 50 ms local completions *and* GPT‑4‑level deep refactors.  
* **Intent Graph** – Keeps requirements ⇆ code in sync, so agents never "hallucinate scope."  
* **Open Extensibility** – Plugin SDK, prompt packs, model mesh; no vendor lock‑in.  
* **Enterprise‑first Trust** – On‑prem option, signed artefacts & full audit trail.

---

## 5 | Unique Value Proposition  
> *"Forge ideas into production‑ready code with the clarity of 3‑D and the power of a senior AI swarm — online or offline."*

---

## 6 | MVP Feature Set

**Core Forge Components**
- Desktop & VS Code plug‑ins
- Quantised 6B local model integration
- 3‑D mini‑map visualization
- Ghost‑branch PR workflow

**Collaboration & XR Features**
- Intent‑Graph panel
- AI swarm personas
- VR/AR code walkthrough capabilities
- Marketplace alpha

**Enterprise Readiness**
- SSO and RBAC implementation
- SOC‑2 compliance pipeline
- Cloud burst to 70B model
- SBOM export functionality
- Private VPC deployment options

### 6.1 MVP Exit / Acceptance Criteria

* **Local Model Performance** < 60 ms p95 for local completions on M4 Max with 7-model strategy.  
* **Model Accuracy** ≥ 92.9% pass@1 on HumanEval (Qwen3-Coder), ≥ 90% on internal EvalPlus suite.
* **Context System** ≤ 50ms context retrieval, ≤ 300MB RAM footprint, ≥ 70% hit rate.
* **Storage Management** Handles 110GB+ model downloads with intelligent cleanup.
* **Model Routing** Intelligent task classification across 7 local + 2 cloud models.
* **Zero‑Trust** mode passes third‑party pentest (full offline operation).  
* **3‑D Viz** renders 1 M‑LOC repo at ≥ 30 FPS on RTX 3070.  
* **Marketplace** publishes/installs plug‑ins via signed manifest.  
* **CI Integration** supports GitHub Actions & GitLab CI templates.

---

### 6.2 | Killer Workflow – Visual-Guided Refactor (VGR)

|               | Detail |
|---------------|--------|
| **Goal** | Slash pull-request merge time by ≥ 70 % for complex refactors through an AI-assisted, 3-D visual workflow. |
| **Primary Persona** | Senior Backend Engineer refactoring legacy modules while maintaining SLA. |
| **Key Metric** | **Time-to-Merge (TTM)** for flagged PRs ≤ 4 h (baseline 16 h). |

#### User Stories
1. **Identify Impact**  
   *As a developer* I can drag-and-drop a file/function in the 3-D call-graph to instantly visualise all upstream/downstream dependencies so that I know the blast radius before editing.

2. **Generate Safe Patch Set**  
   *As a developer* I can click **“Refactor with AI”** and receive a ghost-branch containing:  
   - updated imports & file moves  
   - auto-updated tests  
   - migration notes in `CHANGELOG.md`.

3. **Confidence Gate**  
   *As a reviewer* I see an AI-generated diff summary with risk score, impacted tests, and green CI badge so I can approve in one pass.

#### Functional Requirements
| ID | Requirement |
|----|-------------|
| VGR-F-01 | Real-time G3D scene highlighting affected nodes (< 100 ms on 1 M-LOC repo). |
| VGR-F-02 | AI swarm invokes **Planner → Transformer → Tester → Doc-Bot** sequence. |
| VGR-F-03 | Generates *ghost-branch* PR with linked **Intent-Graph** delta. |
| VGR-F-04 | Intelligent model routing: Qwen3-Coder for previews, Phi-4-mini for agentic tasks, cloud burst (DeepSeek R1) for complex refactors > 4k context. |
| VGR-F-05 | Provides rollback button restoring pre-refactor state. |

#### Non-Functional Requirements
* **Latency**: Visualization update < 150 ms p95.  
* **Accuracy**: 95 % unchanged test pass rate post-merge.  
* **Security**: Operates fully in **Zero-Trust** mode (no outbound net).  

#### Acceptance Criteria
1. Triggering VGR on the sample 50-file legacy module finishes < 3 min incl. tests.  
2. Reviewer approves 80 % of VGR PRs without manual code comments in pilot org.  
3. Post-merge telemetry shows no production regression in 30-day canary window.

#### Risks & Mitigations
| Risk | Mitigation |
|------|------------|
| Scene overload on huge monorepos | Chunked lazy-loading + LOD culling in G3D renderer |
| AI patch hallucination | Two-stage refine: cloud output diff-checks local draft; run unit & type checks before PR |

#### Dependencies
* **Model Infrastructure**: Phase 1.5 model download system for 7 local models (ModelLoader.ts, ModelRouter.ts).
* **Context Engine**: Phase 1.6 Dynamic Context Persistence (FileWatcher.ts, ASTIndexer.ts, VectorDB.ts, SemanticStore.ts).
* **Intelligent Routing**: Model Router with task classification across 9 model endpoints.
* **G3D Rendering**: Incremental scene-graph diffing in G3D (Engine ticket G3D-1201).  
* **AI Swarm**: Swarm orchestrator plug-in API (Core ticket CORE-412).  
* **CI Integration**: In-repo CI templates (DevOps ticket DEV-88).

### 6.3 | Go‑to‑Market Funnel – Extension ➜ Desktop ➜ CLI

| Stage | Surface | Primary CTA / Goal | Key Metrics |
|-------|---------|--------------------|-------------|
| **Discover / Try** | VS Code & NeoVim extension (`ms-aura.vsix`) | Give developers a < 5 MB “taste” of AI swarm and 3‑D mini‑map without leaving their editor. | Conversion to desktop installer, weekly active installs |
| **Adopt / Delight** | **Standalone desktop client** (Electron/Tauri) – *hero experience* | Unlock local 6‑8 GB models, 120 Hz WebGPU, XR walkthroughs, and offline mode. | Daily active users, NPS, model‑latency p95 |
| **Automate / Integrate** | `aura` **CLI** (headless) | Drive CI hooks, Git aliases, and terminal‑editor workflows. | CI minutes saved, script invocations per repo |

> **Launch order:** extension ships first (funnel top), desktop GA follows within 4 weeks, CLI is bundled with desktop but documented for standalone use.

### 6.4 | Engineering Effort Split (MVP 9‑month budget)

| Work‑stream | Effort Share | Rationale |
|-------------|-------------|-----------|
| Core AI coding & refactor | **45 %** | Model orchestration, swarm personas, ghost‑branch PRs |
| 3‑D visualisation (flat) | **25 %** | G3D renderer, call‑graph & intent‑graph |
| Collaboration & marketplace | **15 %** | CRDT presence, plug‑in SDK, in‑app store |
| **XR / VR extras** | **10 %** | VR camera, avatar sync, Quest/Vision Pro shell |
| Compliance & ops plumbing | **5 %** | SOC‑2 pipeline, updater, BYO‑key router guards |

*XR is capped at ~10 % to keep focus on day‑to‑day developer value while still enabling wow‑factor demos.*

## 7 | Technical Architecture

### 7.1 High‑Level Diagram
```
┌─────────────────────────── LOCAL ───────────────────────────┐    ┌─────── CLOUD APIs ──────┐
│                                                             │    │                         │
│  ┌─────┐       ┌──────────────┐      ┌─────────────────┐    │    │  ┌─────────────────┐    │
│  │ IDE │──────▶│ Model Router │─────▶│ 7 Local Models  │    │    │  │ Kimi K2 API     │    │
│  └─────┘       │              │      │ • Qwen3-Coder   │◀───┼────┼─▶│ (Agentic)       │    │
│                │ Task         │      │ • Phi-4-mini    │    │    │  └─────────────────┘    │
│  ┌─────┐       │ Classifier   │      │ • Gemma 3       │    │    │             │           │
│  │ CLI │──────▶│              │      │ • Llama 3.3-70B │    │    │             │           │
│  └─────┘       │              │      │ • Starcoder2    │    │    │  ┌─────────────────┐    │
│                │              │      │ • DeepSeek-V2   │    │    │  │ DeepSeek R1 API │    │
│                │              │      │ • Mistral Dev   │    │    │  │ (Reasoning)     │    │
│                └──────┬───────┘      └─────────────────┘    │    │  └─────────────────┘    │
│                       │                        │           │    │             │           │
│                       ▼                        ▼           │    │             │           │
│  ┌─────────────────────────┐     ┌───────────────────────┐ │    │  ┌─────────────────┐    │
│  │ Dynamic Context Engine  │     │    G3D Renderer      │ │    │  │ BYO-Key APIs    │    │
│  │ • FileWatcher          │     │                       │─┼────┼─▶│ • OpenAI        │    │
│  │ • ASTIndexer           │     │                       │ │    │  │ • Anthropic     │    │
│  │ • VectorDB             │     │                       │ │    │  │ • Google        │    │
│  │ • SemanticStore        │     │                       │ │    │  │ • xAI           │    │
│  │ • ContextPlanner       │     │                       │ │    │  └─────────────────┘    │
│  │ • Retriever            │     │                       │ │    │                         │
│  └─────────────────────────┘     └───────────────────────┘ │    │  ┌─────────────────┐    │
│                       ▲                        │           │    │  │ Encrypted Logs   │    │
│                       │                        │           │    │  │ (Telemetry)     │    │
│  ┌─────────────────────────┐                   │           │    │  └─────────────────┘    │
│  │    Intent DB           │                   │           │    │             ▲           │
│  │                        │                   │           │    │             │           │
│  └─────────────────────────┘                   └───────────┼────┼─────stats───┘           │
│                                                             │    │                         │
└─────────────────────────────────────────────────────────────┘    │  ┌─────────────────┐    │
                                                                   │  │ Plugin Market   │    │
                                                                   │  │                 │    │
                                                                   │  └─────────────────┘    │
                                                                   │             ▲           │
                                                                   └─────────────┼───────────┘
                                                                                 │
                                                                             plugins
                                                                                 │
                                                                    ┌─────────────┘
                                                                    │
                                                                 ┌─────┐
                                                                 │ IDE │
                                                                 └─────┘
```

### 7.2 Front‑end Stack

* **React 18 + TypeScript**
* **Monaco (G3D‑enhanced)** editor
* **WebGPU** G3D renderer
* **Redux Toolkit + Zustand** state
* **WebRTC + Liveblocks** for multi‑user + XR sessions

### 7.3 Back‑end Stack

* Node.js (REST) + **FastAPI** (AI micro‑services)
* PostgreSQL (metadata) & **DuckDB** (telemetry)
* **Git‑FS** storage with delta packing
* Docker + **Kubernetes** (Helm)

### 7.4 **Local & Hybrid AI Model Strategy**

**🎯 Core Model Selection (July 2025)**

We use **7 local model families + 2 cloud APIs** optimized for different scenarios:

**💻 Local Models (Downloaded & Run Locally):**

1. **Qwen3-Coder** (PRIMARY LOCAL) - Default coding assistant
   - Primary workhorse for code completion, generation, and refactoring
   - 4 size variants: 4B (ultra-light), 8B (balanced), 14B (premium), 32B (workstation)
   - Ships with installer, runs offline - 92.9% HumanEval performance

2. **Phi-4-mini** (LOCAL AGENTIC) - Local agentic workflows
   - 3.8B parameters optimized for function calling and tool use
   - Primary local model for autonomous coding tasks
   - Lightweight alternative to cloud agentic APIs

3. **Gemma 3** (LOCAL MULTIMODAL) - Advanced capabilities
   - 3 variants: 4B, 12B, 27B parameters
   - 12B variant includes multimodal capabilities for visual tasks
   - Strong reasoning performance with Google's training

4. **Mistral Devstral Small** (LOCAL CONTEXT) - Extended context
   - 24B parameters with 256K context windows
   - Strong on real-world coding benchmarks (87.5% HumanEval)
   - Apache 2.0 license for commercial use

5. **Llama 3.3-70B** (LOCAL POWER) - Heavy local reasoning
   - 70B parameters for workstation users with 48GB+ VRAM
   - Meta's latest coding model with superior reasoning
   - Alternative to cloud burst for privacy-critical scenarios

6. **Starcoder2-15B** (LOCAL POLYGLOT) - Multi-language specialist
   - 15B parameters supporting 600+ programming languages
   - Optimized for polyglot programming and code translation
   - Excellent for diverse tech stacks

7. **DeepSeek-Coder V2 Lite** (LOCAL EFFICIENT) - Fast inference
   - 16B parameters with MoE (Mixture of Experts) architecture
   - Optimized for fast inference with high efficiency
   - Alternative coding model with competitive performance

**☁️ Cloud APIs (aura Managed):**

8. **Kimi K2** (CLOUD AGENTIC) - Advanced agentic workflows
   - 1T parameter MoE model (32B active) optimized for tool use
   - 53.7% LiveCodeBench, 65.8% SWE-bench performance
   - Cost-effective at $0.60/$2.50 per M tokens

9. **DeepSeek R1** (CLOUD REASONING) - Complex reasoning
   - 671B parameter model with advanced reasoning capabilities
   - Full codebase understanding and large refactors
   - Cloud burst when context >4k tokens or complexity high

**🎯 Summary - Complete Model Matrix:**

| Model | Purpose | Deployment | Size | Why We Chose It |
|-------|---------|------------|------|-----------------|
| **Qwen3-Coder** | Primary coding | Local (auto-install) | 4B/8B/14B/32B | Best HumanEval scores, 4 size options |
| **Phi-4-mini** | Local agentic | Local (optional) | 3.8B | Function calling, lightweight agentic |
| **Gemma 3** | Multimodal tasks | Local (optional) | 4B/12B/27B | Google's multimodal capabilities |
| **Mistral Devstral** | Long context | Local (optional) | 24B | 256K context, strong benchmarks |
| **Llama 3.3-70B** | Heavy reasoning | Local (optional) | 70B | Meta's latest, workstation power |
| **Starcoder2** | Polyglot coding | Local (optional) | 15B | 600+ languages, code translation |
| **DeepSeek-Coder V2** | Fast inference | Local (optional) | 16B MoE | Efficient MoE architecture |
| **Kimi K2** | Cloud agentic | Cloud API | 1T (32B active) | Superior autonomous workflows |
| **DeepSeek R1** | Cloud reasoning | Cloud API | 671B | Advanced reasoning capabilities |

**🔑 BYO-Key APIs (User Provides Keys):**
- **OpenAI**: GPT-4.1, o3-mini for premium completions
- **Anthropic**: Claude 4 Opus/Sonnet for best reasoning  
- **Google**: Gemini 2.5 Pro with 2M context window
- **xAI**: Grok 4 for real-time information
- **Meta**: Llama 4 70B via cloud providers
- **Custom**: Any OpenAI-compatible API endpoint

**Orchestration Logic**

1. ***Code completion*** → Qwen3-Coder (local, size based on hardware) for suggestions < 60 ms.
2. ***Local agentic tasks*** → Phi-4-mini (local) for function calling and autonomous workflows.
3. ***Cloud agentic tasks*** → Kimi K2 (cloud API) for complex multi-step workflows.
4. ***Complex reasoning*** → DeepSeek R1 (cloud) when context >4k tokens or architectural decisions.
5. ***Multimodal tasks*** → Gemma 3-12B (local) or cloud APIs for visual/diagram processing.
6. ***Polyglot programming*** → Starcoder2-15B (local) for 600+ language support.
7. ***Long context (256K)*** → Mistral Devstral (local) or Gemini 2.5 Pro (cloud).
8. ***Heavy local reasoning*** → Llama 3.3-70B (local, workstation only) for privacy-critical scenarios.
9. ***Fast inference*** → DeepSeek-Coder V2 Lite (local) for efficient MoE processing.

---

### 7.4.1 Model Download & Installation Strategy

**📦 What Gets Downloaded and When?**

| Model | When | How | Storage | Use Case |
|-------|------|-----|---------|----------|
| **Qwen3-Coder 14B** | During install | Automatic (required) | 7.8 GB | Default coding assistant |
| **Qwen3-Coder 8B** | On-demand | Settings → Models → Install | 4.5 GB | Lower-spec machines |
| **Qwen3-Coder 4B** | On-demand | Settings → Models → Install | 2.2 GB | Ultra-light laptops |
| **Qwen3-Coder 32B** | On-demand | Settings → Models → Install | 18 GB | Workstation performance |
| **Phi-4-mini** | Auto-install | Bundled with installer | 2.8 GB | Local agentic workflows |
| **Gemma 3-4B** | On-demand | Settings → Models → Install | 3.2 GB | Lightweight multimodal |
| **Gemma 3-12B** | On-demand | Settings → Models → Install | 8.1 GB | Advanced multimodal |
| **Gemma 3-27B** | On-demand | Settings → Models → Install | 16.5 GB | High-end multimodal |
| **Mistral Devstral Small** | On-demand | Settings → Models → Install | 14.2 GB | Long context (256K) |
| **Llama 3.3-70B** | On-demand | Settings → Models → Install | 35 GB | Heavy local reasoning |
| **Starcoder2-15B** | On-demand | Settings → Models → Install | 8.7 GB | Polyglot programming |
| **DeepSeek-Coder V2 Lite** | On-demand | Settings → Models → Install | 9.6 GB | Fast MoE inference |
| **Kimi K2** | Never | Cloud API only | 0 GB | Cloud agentic workflows |
| **DeepSeek R1** | Never | Cloud API only | 0 GB | Complex reasoning |

**🔄 Download Process:**
- Models fetched from **Hugging Face** using `llama.cpp` compatible GGUF format
- **Chunked downloads** with resume capability (no restart on connection loss)
- **SHA-256 verification** ensures model integrity
- **Background downloads** - keep coding while models fetch
- **Delta updates** - only download changed layers on model updates

**💾 Typical Storage Requirements:**

| User Profile | Models | Total Disk | GPU VRAM | Use Case |
|--------------|---------|------------|----------|----------|
| **Minimal** | Qwen3-4B + Phi-4-mini | 5 GB | 4 GB | Ultra-light setup |
| **Standard** | Qwen3-14B + Phi-4-mini | 10.6 GB | 8 GB | Default recommended setup |
| **Power User** | Qwen3-14B + Phi-4-mini + Gemma-12B + Starcoder2 | 27.2 GB | 16 GB | Multi-task capabilities |
| **Enthusiast** | Qwen3-32B + Multiple variants | 60 GB | 24 GB | High-performance local |
| **Workstation** | Llama-70B + Full suite | 110+ GB | 48+ GB | Maximum local capabilities |
| **Corporate** | Qwen3-4B + Cloud APIs | 5 GB | 4 GB | Light local + cloud burst |

**🚀 Model Selection by Hardware:**

| Your GPU | Recommended Setup | Experience |
|----------|-------------------|------------|
| **RTX 3060 (12GB)** | Qwen3-14B + Phi-4-mini | Smooth completions + local agentic |
| **RTX 3090 (24GB)** | Qwen3-14B + Phi-4-mini + Gemma-12B + Starcoder2 | Multi-model capabilities |
| **RTX 4070 (12GB)** | Qwen3-14B + Phi-4-mini + Mistral Devstral | Balanced performance |
| **RTX 4090 (24GB)** | Qwen3-32B + Multiple models | High-end local inference |
| **H100 (80GB)** | Llama-70B + Full suite | Maximum local power |
| **M2/M3 Mac (16GB)** | Qwen3-8B + Phi-4-mini + Cloud | Optimized for Apple Silicon |
| **Laptop (8GB)** | Qwen3-4B + Cloud APIs | Ultra-light local assist |

**📊 When Each Model Gets Used:**

```
Input → Task Classification → Model Selection
├─ Code completion → Qwen3-Coder (local, size based on hardware)
├─ Local agentic task → Phi-4-mini (local function calling)
├─ Cloud agentic task → Kimi K2 (cloud API)
├─ Complex reasoning → DeepSeek R1 (cloud, >4K tokens)
├─ Multimodal task → Gemma 3-12B (local) or cloud APIs
├─ Polyglot programming → Starcoder2-15B (local)
├─ Long context (256K) → Mistral Devstral (local) or Gemini 2.5 Pro (cloud)
├─ Heavy reasoning (privacy) → Llama 3.3-70B (local, workstation)
└─ Fast MoE inference → DeepSeek-Coder V2 Lite (local)
```

---

## 7.5 | Instrumentation & Experimentation

* **Event Schema** — PageView, SuggestionAccepted, CloudBurst, VizFrameRate.  
* **Privacy** — Events anonymised w/ differential privacy; opt‑out toggle.  
* **A/B Harness** — Flag‑based rollouts (e.g., new prompt strategy) via OpenFeature SDK.  
* **North‑Star Metric** — "Time‑to‑Merge" from first code edit to PR merge.

---

## 7.6 | Model Evaluation

| Benchmark | Metric | Target | Cadence |
|-----------|--------|--------|---------|
| **HumanEval+ 164** | pass@1 | ≥ 80 % local / ≥ 90 % cloud | Nightly |
| **SWE‑bench** | solved % | ≥ 35 % cloud | Weekly |
| **CS‑Eval (CompSci Q‑A)** | accuracy | ≥ 75 % local | Monthly |

Open‑source leaderboard scripts will run inside GitHub Actions, posting badged results to README.

---

## 7.7 | Compliance Matrix

| Domain | Requirement | Design Measure |
|--------|-------------|----------------|
| **EU AI Act** | Risk management, transparency | Model cards + audit log export |
| **SOC 2 Type II** | Security, Availability | Change‑control + disaster‑recovery runbooks |
| **FedRAMP Low** | Gov pilot | AWS GovCloud template, FIPS TLS |

---

## 7.8 | Threat Model Highlights

| Category | Example Threat | Mitigation |
|----------|----------------|------------|
| **Spoofing** | Malicious plug‑in masquerades | Signed marketplace, key pinning |
| **Tampering** | Model weight injection | SHA‑256 integrity + secure boot |
| **Repudiation** | Dev denies code change | Immutable ghost‑branch history |
| **Information Disclosure** | Cloud logging leak | Per‑tenant KMS keys |
| **DoS** | Prompt bombs | Rate & token‑limiters per org |
| **Elevation** | Priv‑escalation in agent shell | Seccomp jailed subprocess |

---

## 7.9 | Open‑Source Strategy

* **Core Closed‑Source Modules** — Swarm orchestrator, 3‑D renderer shaders.  
* **Apache‑2.0 SDK** — Plug‑in API, model mesh adapters.  
* **Dual‑Licensing** — GPL fusion detection via FOSSology; SPDX manifests auto‑generated.

---

## 7.10 | Marketplace Governance

1. **Submission Review** — Static scan, signature, automated tests (≤ 2 h SLA).  
2. **Revenue Split** — 80 % creator / 20 % aura; payouts via Stripe Connect.  
3. **Takedown Process** — 24‑hour security/DMCA response window.  
4. **Quality Ranking** — TrustScore combines install volume + crash telemetry.

---

## 7.11 | Performance & Scalability Targets

* **Local Memory Footprint** ≤ 1.2× repo size.  
* **Cloud Inference TPS** — 20 req/s per A100 with batching.  
* **Horizontal Scale** — K8s HPA on GPU utilisation > 60 %.  
* **SLOs** — 99.5 % ≤ 500 ms P90 /suggest API.

---

## 7.12 | Accessibility / Internationalisation

* 3‑D colour palettes WCAG AA contrast.  
* Screen‑reader‑friendly diff views (ARIA live regions).  
* Prompt packs localised (EN, ZH, ES, PT, HI) using ICU message format.

---

## 7.13 | Developer Environment

* **One‑Command Dev** — `make dev` spins Docker compose (Postgres, Minio, Superset).  
* **Git Hooks** — Pre‑commit Black & ESLint; pre‑push model eval smoke.  
* **Seed Scripts** — Demo repo + synthetic bug tickets for sandbox demos.

---

## 7.5 | Dynamic Context Persistence System 🆕

**🎯 Goal:** Persistent, real-time code context so local LLMs never "forget" the project.

### Memory Foundation (Phase 1.6)

| Component | Purpose | Performance Target |
|-----------|---------|-------------------|
| **FileWatcher.ts** | Cross-platform FS watchers with debounced events | < 10ms event processing |
| **ASTIndexer.ts** | Incremental Tree-sitter diff parser → symbol & embedding queue | < 200ms incremental updates |
| **VectorDB.ts** | Local Qdrant/Faiss wrapper for K-NN search | < 50ms vector retrieval |
| **SemanticStore.ts** | Multi-tier store (hot in-mem, warm SQLite, cold Parquet) | < 100ms semantic queries |

### Context Engine GA (Phase 2.0)

| Component | Purpose | Integration |
|-----------|---------|-------------|
| **ContextPlanner.ts** | Classify IDE/CLI events → intent labels | Hooks into editor events |
| **Retriever.ts** | Hybrid relevance ranker (BM25 + cosine + recency) | Assembles context for LLM calls |
| **PromptAssembler.ts** | Builds final prompt with compression & safety margin | Injects into model requests |
| **MemoryPanel.tsx** | Shows "live context" chunks, tokens, pin/unpin UI | Real-time context visualization |
| **MemorySettings.tsx** | Disk quota, expiry, privacy toggles | User control over memory system |

### Memory Feedback Loop (Phase 3)

**RelevanceLearner.ts** - Online learning from user feedback:
- 👍/👎 buttons update chunk relevance scores
- Merge success metrics improve context selection
- Multi-armed bandit optimization of ranking weights

### Performance Specifications

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Context Retrieval Latency** | ≤ 50ms p95 | Planner + Retriever + Assembler pipeline |
| **Extra RAM Footprint** | ≤ 300MB | At 100k LOC indexed with hot cache |
| **Disk Overhead** | ≤ 1% | Cold + warm tiers vs raw repo size |
| **Context Hit Rate** | ≥ 70% | Relevant context chunks retrieved |
| **LLM Suggestion Validity** | +15pp | Improvement vs baseline without context |

### Key Innovation Benefits

1. **Never Loses Context** - Local LLMs maintain project awareness across sessions
2. **Intelligent Relevance** - ML-powered ranking learns from developer feedback  
3. **Privacy-First** - All context processing happens locally
4. **Real-Time Updates** - Sub-250ms incremental indexing on file saves
5. **Multi-Tier Storage** - Hot/warm/cold optimization for large codebases

---

## 8 | Strategic Model‑Vendor Integration Plan

> Goal: deliver "best‑available" intelligence on Day 1 while protecting aura from single‑vendor lock‑in or sudden price spikes.

### 1. Multi‑Vendor Abstraction ("Model Mesh")

| Adapter | Primary Strength | Typical Use‑Case | Current SLA* |
|---------|------------------|------------------|--------------|
| **OpenAI (GPT‑4.1/o3)** | Highest raw accuracy & advanced reasoning | Complex refactors, doc generation | 99.9 % / 40 k TPM |
| **Anthropic Claude 4 Sonnet/Haiku** | Very long context (200 k) & safety, thinking variants | Large repo Q&A, policy checks | 99.9 % / 40 k TPM |
| **Google Gemini 2.5 Pro** | Multimodal + 1M context + inline execution | Architecture sketches, UML auto‑draw | 99.95 % / 60 k TPM |
| **xAI Grok 4** | Fast reasoning, 256k context, OS‑friendly terms | Exploratory brainstorming, bug hunts | 99.8 % / 30 k TPM |
| **DeepSeek R1 671B (self‑host)** | SOTA open code model with reasoning | Privacy‑critical cloud burst | N/A (self‑ops) |

\*SLAs based on vendor documentation July 2025.

An internal **LLM Router** ranks adapters by *Cost × Latency × Quality* for each request class ("completion", "analysis", "vision", etc.), with user/org policy overrides.

### 2. BYO‑Key & Bring‑Your‑Own‑Model

* **Pro & Enterprise users** can plug their own OpenAI / Claude / Gemini keys.  
* **On‑Prem option** allows customers to swap in a private or fine‑tuned model behind our gRPC shim (e.g., IBM‑Watson‑Code, local Qwen‑72B).

#### 8.2.1 Commercial Guard‑Rails for BYO & On‑Prem

| Lever | What it does | Revenue / Margin Impact |
|-------|--------------|-------------------------|
| **Token ceilings** | Dev/Team tiers include daily cloud tokens; overages trigger pay‑as‑you‑go blocks or BYO prompt. | Protects gross margin on high‑usage accounts |
| **Smart‑Router+ analytics add‑on** | \$10‑20/seat for cost‑optimisation dashboards, cache hit‑rates, vendor SLA alerts. | New ARR line, upsell path |
| **Premium swarm personas** | Security‑Sweep & SBOM signer run only on aura‑hardened models. | Ensures some paid tokens even for BYO orgs |
| **Runtime licence for on‑prem GPUs** | \$5 k/GPU/year for DeepSeek R1‑671B container key. | Captures value from self‑hosted clusters |
| **Support SLAs** | 9×5 email (included), 24×7 pager (Enterprise Plus). | Service revenue & enterprise confidence |

These guard‑rails let us embrace BYO without capping upside, while keeping COGS predictable.

### 3. Negotiation & Partnership Tracks

| Vendor | Current Status | Q3‑2025 Objectives |
|--------|----------------|--------------------|
| **OpenAI** | Tier‑1 Build Partner; 3 M free tokens/mo | Secure *pilot‑partner* pricing; GPT-4.1/o3 optimization |
| **Anthropic** | Dev‑Program seat | Access Claude 4 Sonnet/Opus fine‑tune, volume discount >= 100 M tokens/month |
| **Google** | Cloud credits via AI Startup Program | Gemini 2.5 Pro integration for advanced reasoning tier |
| **xAI** | In discussion | GPU co‑location deal for Grok‑Edge inferencing |
| **AWS Bedrock** | On‑hold | Evaluate when CodeWhisperer‑Next public |

### 4. Data Governance & Compliance

* **No vendor retains customer code** → `X-Data-Policy: opt-out-training` header where supported.  
* **Double‑encryption in transit** — mTLS to vendor endpoint plus payload encryption for highly‑regulated clients.  
* **Regional routing** — EU traffic pinned to EU model endpoints (GDPR / EU‑AI‑Act).  

### 5. Cost & Fail‑over Logic

1. **Primary** route to cheapest model meeting *Quality ≥ Target* (e.g., Claude Haiku for large‑context doc chat).  
2. **Fallback A** — next provider in ranking if latency > 1 s or HTTP 5xx.  
3. **Fallback B** — self‑hosted DeepSeek R1 (auto‑scale GPU pods) when external vendors down.  
4. **Budget Guardrails** — org‑level token ceilings + Slack‑alert when 80 % reached.

### 6. API Surface (Developer Facing)

```yaml
POST /forge/ai/invoke
{
  "task": "code.review",
  "modelPref": ["local", "openai:gpt-4.1", "anthropic:claude-4-sonnet"],
  "temperature": 0.2,
  "sourceFiles": ["*.ts", "!tests/**"]
}
```

*Router returns `modelUsed` and `costUSD` for transparency.*

---

## 9 | Business Model & Pricing

| Plan               | Price           | Target        | Key Limits                     |
| ------------------ | --------------- | ------------- | ------------------------------ |
| **Developer**      | \$39/mo         | Indies        | 15 k cloud tokens/day          |
| **Team**           | \$99/mo         | 3‑50 dev orgs | Unlimited cloud; XR collab     |
| **Enterprise**     | \$299/mo        | 50‑5 000 devs | Private VPC, audit, SSO        |
| **G3D Enterprise** | from \$100 k/yr | Fortune‑500   | On‑prem cluster, custom models |

Marketplace revenue split 80/20 (creator/aura).

---

## 10 | KPIs

### Core Performance Metrics
* **< 60 ms** local inference latency (95p)
* **< 2 s** cloud task latency (95p)
* **95 %+** HumanEval pass@1 on cloud tier
* **92.9 %+** HumanEval pass@1 on local Qwen3-Coder

### Context & Memory Performance 🆕
* **≤ 50 ms** context retrieval latency (95p)
* **≤ 300 MB** extra RAM footprint (100k LOC)
* **≤ 1 %** disk overhead vs repo size
* **≥ 70 %** context hit rate
* **+15 pp** LLM suggestion validity improvement

### Business Metrics
* **4.9 / 5** developer NPS
* **< 2 %** monthly churn
* **> 92 %** gross margin
* **99.99 %** uptime

---

## 11 | Risks & Mitigations

| Risk               | Impact | Mitigation                                            |
| ------------------ | ------ | ----------------------------------------------------- |
| Model supply drift | Medium | Maintain model mesh; hot‑swap OSS weights             |
| GPU cost spikes    | High   | Local‑first default; MoE sparsity; batch distillation |
| Data breach        | High   | Client‑side encryption; zero‑telemetry mode           |
| UX complexity      | Medium | Progressive‑reveal UI; user studies & A/B tests       |
| OSS licence clash  | Low    | Vet third‑party models; SPDX tracking                 |

---

## 12 | Next Steps

### Immediate Actions (Phase 1 Completion)
1. **Complete Model Infrastructure** - Finish Phase 1.5 model download system for 7 local models.
2. **Implement Context Engine** - Deploy Phase 1.6 Dynamic Context Persistence (4 core services).
3. **Model Integration Testing** - Benchmark all 7 local models + 2 cloud APIs on target hardware.
4. **Storage Optimization** - Implement intelligent model management for 110GB+ downloads.

### Foundation Setup (Phases 1.6-2.0)
5. **Context System GA** - Deploy Phase 2.0 Context Engine with MemoryPanel and MemorySettings.
6. **Performance Validation** - Achieve ≤50ms context retrieval and ≤300MB RAM targets.
7. **Model Router Optimization** - Fine-tune intelligent routing across 9 model endpoints.

### Strategic Initiatives 
8. **Design Partner Program** - Secure 2 Fortune-500 enterprise pilots for context system.
9. **UX Research** - Run workshops for 3D visual metaphors & 7-model selection UX.
10. **Marketplace Foundation** - Establish plugin SDK and early extension author program.

---

*Prepared by Product Strategy – aura, July 2025*