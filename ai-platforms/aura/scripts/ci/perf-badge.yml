# Aura CI Performance Validation & Badge Generation
# Automated benchmark suite with performance regression gates

name: 'Performance Validation & Badge Generation'

on:
  push:
    branches: [main, beta, develop]
  pull_request:
    branches: [main, beta]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      benchmark_suite:
        description: 'Benchmark suite to run'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - basic
          - regression-only
          - custom

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  BENCHMARK_TIMEOUT: '30m'
  
jobs:
  # Performance Benchmark Suite
  performance-benchmarks:
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        node-version: ['18', '20']
        
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Need full history for trend analysis
          
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache Dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            ~/.cache/pip
            node_modules
            **/.venv
          key: ${{ runner.os }}-deps-${{ hashFiles('**/package-lock.json', '**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-deps-
            
      - name: Install Dependencies
        run: |
          npm ci
          pip install -r requirements.txt
          npm run setup:benchmarks
          
      - name: Setup Test Environment
        run: |
          # Create test repositories for benchmarking
          mkdir -p benchmark-repos
          cd benchmark-repos
          
          # Small repo (1K LOC)
          git clone --depth 1 https://github.com/microsoft/TypeScript-Node-Starter.git small-repo
          
          # Medium repo (10K LOC)  
          git clone --depth 1 https://github.com/microsoft/vscode.git medium-repo
          
          # Large repo (100K LOC)
          git clone --depth 1 https://github.com/microsoft/TypeScript.git large-repo
          
      - name: Run Core Performance Benchmarks
        id: core-benchmarks
        run: |
          npm run benchmark:core -- \
            --format json \
            --output benchmark-results/core-${{ matrix.os }}-${{ matrix.node-version }}.json \
            --timeout ${{ env.BENCHMARK_TIMEOUT }}
            
      - name: Run Model Inference Benchmarks
        id: model-benchmarks
        run: |
          npm run benchmark:models -- \
            --models "qwen-3-coder,phi-3.5-mini" \
            --format json \
            --output benchmark-results/models-${{ matrix.os }}-${{ matrix.node-version }}.json
            
      - name: Run 3D Visualization Benchmarks
        id: 3d-benchmarks
        run: |
          npm run benchmark:3d -- \
            --repos benchmark-repos/* \
            --target-fps 30 \
            --format json \
            --output benchmark-results/3d-${{ matrix.os }}-${{ matrix.node-version }}.json
            
      - name: Run Memory Usage Benchmarks
        id: memory-benchmarks
        run: |
          npm run benchmark:memory -- \
            --duration 300 \
            --format json \
            --output benchmark-results/memory-${{ matrix.os }}-${{ matrix.node-version }}.json
            
      - name: Run Integration Benchmarks
        id: integration-benchmarks
        run: |
          npm run benchmark:integration -- \
            --scenarios "completion,3d-navigation,context-retrieval" \
            --format json \
            --output benchmark-results/integration-${{ matrix.os }}-${{ matrix.node-version }}.json
            
      - name: Archive Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results-${{ matrix.os }}-${{ matrix.node-version }}
          path: benchmark-results/
          retention-days: 90
          
      - name: Validate Performance Thresholds
        id: threshold-validation
        run: |
          npm run validate:thresholds -- \
            --results benchmark-results/ \
            --thresholds scripts/ci/performance-thresholds.json \
            --fail-on-regression
            
  # Demo Repository Performance Testing
  demo-repository-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Test Environment
        run: |
          npm ci
          npm run build:demo
          
      - name: Setup Demo Repositories
        run: |
          mkdir -p demo-repos
          cd demo-repos
          
          # Popular open source projects for testing
          git clone --depth 1 https://github.com/facebook/react.git react
          git clone --depth 1 https://github.com/vuejs/vue.git vue
          git clone --depth 1 https://github.com/angular/angular.git angular
          git clone --depth 1 https://github.com/microsoft/TypeScript.git typescript
          git clone --depth 1 https://github.com/nodejs/node.git node
          
      - name: Run Demo Performance Tests
        id: demo-tests
        run: |
          npm run test:demo-performance -- \
            --repos demo-repos/* \
            --metrics "indexing-time,completion-latency,3d-render-fps" \
            --output demo-results.json
            
      - name: Generate Demo Report
        run: |
          npm run generate:demo-report -- \
            --input demo-results.json \
            --output demo-performance-report.html \
            --format html
            
      - name: Upload Demo Report
        uses: actions/upload-artifact@v3
        with:
          name: demo-performance-report
          path: demo-performance-report.html
          
  # Performance Regression Detection
  regression-analysis:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks]
    timeout-minutes: 15
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 50 # Need history for regression analysis
          
      - name: Download Benchmark Results
        uses: actions/download-artifact@v3
        with:
          path: current-results/
          
      - name: Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install Analysis Tools
        run: |
          pip install pandas numpy scipy matplotlib seaborn
          pip install -r scripts/ci/analysis-requirements.txt
          
      - name: Download Historical Data
        run: |
          # Download previous benchmark results from cache or artifact storage
          python scripts/ci/download-historical-benchmarks.py \
            --output historical-results/ \
            --days 30
            
      - name: Perform Regression Analysis
        id: regression-analysis
        run: |
          python scripts/ci/analyze-performance-regression.py \
            --current current-results/ \
            --historical historical-results/ \
            --output regression-analysis.json \
            --threshold 10 # 10% regression threshold
            
      - name: Generate Regression Report
        if: always()
        run: |
          python scripts/ci/generate-regression-report.py \
            --analysis regression-analysis.json \
            --output regression-report.html \
            --format html
            
      - name: Check Regression Gates
        run: |
          python scripts/ci/check-regression-gates.py \
            --analysis regression-analysis.json \
            --fail-on-major-regression
            
      - name: Upload Regression Report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: regression-analysis-report
          path: regression-report.html
          
  # Cross-Platform Benchmark Validation
  cross-platform-validation:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks]
    timeout-minutes: 20
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Platform Results
        uses: actions/download-artifact@v3
        with:
          path: platform-results/
          
      - name: Setup Analysis Environment
        run: |
          npm ci
          pip install pandas numpy matplotlib
          
      - name: Cross-Platform Analysis
        id: platform-analysis
        run: |
          python scripts/ci/analyze-cross-platform.py \
            --results platform-results/ \
            --output platform-analysis.json \
            --platforms "ubuntu-latest,windows-latest,macos-latest"
            
      - name: Validate Platform Consistency
        run: |
          python scripts/ci/validate-platform-consistency.py \
            --analysis platform-analysis.json \
            --max-variance 25 # 25% maximum variance between platforms
            
      - name: Generate Platform Report
        run: |
          python scripts/ci/generate-platform-report.py \
            --analysis platform-analysis.json \
            --output platform-consistency-report.html
            
  # Performance Trend Analysis
  trend-analysis:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks]
    if: github.ref == 'refs/heads/main'
    timeout-minutes: 15
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Current Results
        uses: actions/download-artifact@v3
        with:
          path: current-results/
          
      - name: Setup Analysis Environment
        run: |
          pip install pandas numpy matplotlib seaborn plotly
          
      - name: Download Extended Historical Data
        run: |
          python scripts/ci/download-historical-benchmarks.py \
            --output historical-results/ \
            --days 90 \
            --include-releases
            
      - name: Generate Trend Analysis
        id: trend-analysis
        run: |
          python scripts/ci/analyze-performance-trends.py \
            --current current-results/ \
            --historical historical-results/ \
            --output trend-analysis.json \
            --generate-charts
            
      - name: Create Trend Dashboard
        run: |
          python scripts/ci/create-trend-dashboard.py \
            --analysis trend-analysis.json \
            --output performance-dashboard.html \
            --interactive
            
      - name: Store Results for Future Analysis
        run: |
          python scripts/ci/store-benchmark-results.py \
            --results current-results/ \
            --storage-backend github-cache \
            --retention-days 90
            
  # Generate Performance Badges
  generate-badges:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks, regression-analysis, cross-platform-validation]
    if: always() && github.ref == 'refs/heads/main'
    timeout-minutes: 10
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Results
        uses: actions/download-artifact@v3
        with:
          path: all-results/
          
      - name: Setup Badge Generation
        run: |
          npm install badge-maker axios
          pip install requests jinja2
          
      - name: Calculate Performance Metrics
        id: metrics
        run: |
          python scripts/ci/calculate-badge-metrics.py \
            --results all-results/ \
            --output badge-metrics.json
            
      - name: Generate Performance Badges
        run: |
          node scripts/ci/generate-performance-badges.js \
            --metrics badge-metrics.json \
            --output badges/
            
      - name: Update Shields.io Badges
        env:
          SHIELDS_IO_TOKEN: ${{ secrets.SHIELDS_IO_TOKEN }}
        run: |
          # Update completion latency badge
          LATENCY=$(jq -r '.completion_latency_p95' badge-metrics.json)
          curl -X POST "https://img.shields.io/endpoint" \
            -H "Authorization: Bearer $SHIELDS_IO_TOKEN" \
            -d @- <<EOF
          {
            "schemaVersion": 1,
            "label": "completion latency",
            "message": "${LATENCY}ms",
            "color": "$( [ ${LATENCY%ms} -lt 60 ] && echo green || [ ${LATENCY%ms} -lt 100 ] && echo yellow || echo red )"
          }
          EOF
          
          # Update 3D FPS badge
          FPS=$(jq -r '.fps_average' badge-metrics.json)
          curl -X POST "https://img.shields.io/endpoint" \
            -H "Authorization: Bearer $SHIELDS_IO_TOKEN" \
            -d @- <<EOF
          {
            "schemaVersion": 1,
            "label": "3D FPS",
            "message": "${FPS} fps",
            "color": "$( [ ${FPS} -gt 30 ] && echo green || [ ${FPS} -gt 20 ] && echo yellow || echo red )"
          }
          EOF
          
          # Update memory efficiency badge  
          MEMORY=$(jq -r '.memory_efficiency' badge-metrics.json)
          curl -X POST "https://img.shields.io/endpoint" \
            -H "Authorization: Bearer $SHIELDS_IO_TOKEN" \
            -d @- <<EOF
          {
            "schemaVersion": 1,
            "label": "memory efficiency",
            "message": "${MEMORY}%",
            "color": "$( [ ${MEMORY} -gt 90 ] && echo green || [ ${MEMORY} -gt 80 ] && echo yellow || echo red )"
          }
          EOF
          
      - name: Generate README Badges
        run: |
          python scripts/ci/update-readme-badges.py \
            --metrics badge-metrics.json \
            --readme README.md \
            --backup
            
      - name: Commit Badge Updates
        if: github.ref == 'refs/heads/main'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add badges/ README.md
          git diff --staged --quiet || git commit -m "ðŸ“Š Update performance badges [skip ci]"
          git push
          
  # Performance Report Generation
  generate-reports:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks, demo-repository-tests, regression-analysis, trend-analysis]
    if: always()
    timeout-minutes: 15
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Artifacts
        uses: actions/download-artifact@v3
        with:
          path: artifacts/
          
      - name: Setup Report Generation
        run: |
          pip install jinja2 matplotlib plotly pandas weasyprint
          npm install puppeteer
          
      - name: Generate Comprehensive Performance Report
        run: |
          python scripts/ci/generate-comprehensive-report.py \
            --artifacts artifacts/ \
            --template scripts/ci/templates/performance-report.html.j2 \
            --output comprehensive-performance-report.html \
            --include-charts \
            --include-recommendations
            
      - name: Generate Executive Summary
        run: |
          python scripts/ci/generate-executive-summary.py \
            --artifacts artifacts/ \
            --output executive-summary.pdf \
            --format pdf
            
      - name: Upload Performance Reports
        uses: actions/upload-artifact@v3
        with:
          name: performance-reports
          path: |
            comprehensive-performance-report.html
            executive-summary.pdf
          retention-days: 180
          
  # Notification and Integration
  notify-results:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks, regression-analysis, generate-badges]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
      - name: Determine Overall Status
        id: status
        run: |
          # Logic to determine if performance tests passed/failed
          BENCHMARK_STATUS="${{ needs.performance-benchmarks.result }}"
          REGRESSION_STATUS="${{ needs.regression-analysis.result }}"
          
          if [ "$BENCHMARK_STATUS" = "success" ] && [ "$REGRESSION_STATUS" = "success" ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=All performance tests passed âœ…" >> $GITHUB_OUTPUT
          elif [ "$BENCHMARK_STATUS" = "failure" ] || [ "$REGRESSION_STATUS" = "failure" ]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=Performance tests failed âŒ" >> $GITHUB_OUTPUT
          else
            echo "status=warning" >> $GITHUB_OUTPUT
            echo "message=Performance tests completed with warnings âš ï¸" >> $GITHUB_OUTPUT
          fi
          
      - name: Notify Slack
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              "text": "Aura Performance Test Results",
              "attachments": [
                {
                  "color": "${{ steps.status.outputs.status == 'success' && 'good' || steps.status.outputs.status == 'failure' && 'danger' || 'warning' }}",
                  "fields": [
                    {
                      "title": "Status",
                      "value": "${{ steps.status.outputs.message }}",
                      "short": true
                    },
                    {
                      "title": "Commit",
                      "value": "${{ github.sha }}",
                      "short": true
                    },
                    {
                      "title": "Reports",
                      "value": "View detailed results in GitHub Actions artifacts",
                      "short": false
                    }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          
      - name: Update GitHub Status
        uses: actions/github-script@v6
        with:
          script: |
            const { owner, repo } = context.repo;
            const sha = context.sha;
            
            await github.rest.repos.createCommitStatus({
              owner,
              repo,
              sha,
              state: '${{ steps.status.outputs.status == 'success' && 'success' || 'failure' }}',
              target_url: `${context.payload.repository.html_url}/actions/runs/${context.runId}`,
              description: '${{ steps.status.outputs.message }}',
              context: 'ci/performance-validation'
            });

# Cleanup and Maintenance
cleanup:
  runs-on: ubuntu-latest
  if: always()
  
  steps:
    - name: Cleanup Old Artifacts
      uses: actions/github-script@v6
      with:
        script: |
          const { owner, repo } = context.repo;
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner,
            repo,
            per_page: 100
          });
          
          const cutoff = new Date();
          cutoff.setDate(cutoff.getDate() - 30); // Keep 30 days
          
          for (const artifact of artifacts.data.artifacts) {
            if (new Date(artifact.created_at) < cutoff) {
              await github.rest.actions.deleteArtifact({
                owner,
                repo,
                artifact_id: artifact.id
              });
            }
          } 