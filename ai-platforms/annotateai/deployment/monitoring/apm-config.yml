---
# Jaeger for distributed tracing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-all-in-one
  namespace: annotateai
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.50
        ports:
        - containerPort: 16686
          name: ui
        - containerPort: 14268
          name: collector
        - containerPort: 6831
          name: agent-udp
        - containerPort: 6832
          name: agent-binary
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 100m
            memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-service
  namespace: annotateai
spec:
  selector:
    app: jaeger
  ports:
  - name: ui
    port: 16686
    targetPort: 16686
  - name: collector
    port: 14268
    targetPort: 14268
  - name: agent-udp
    port: 6831
    targetPort: 6831
    protocol: UDP
  - name: agent-binary
    port: 6832
    targetPort: 6832
---
# OpenTelemetry Collector
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: annotateai
data:
  otel-collector.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: 'annotateai-metrics'
            static_configs:
            - targets: ['annotateai-api-gateway:8080']
    
    processors:
      batch:
        timeout: 5s
        send_batch_size: 1024
      memory_limiter:
        check_interval: 1s
        limit_mib: 512
      resource:
        attributes:
        - key: service.name
          value: annotateai
          action: upsert
        - key: service.version
          value: "1.0.0"
          action: upsert
    
    exporters:
      jaeger:
        endpoint: jaeger-service:14250
        tls:
          insecure: true
      prometheus:
        endpoint: "0.0.0.0:8889"
        namespace: annotateai
        const_labels:
          platform: "annotateai"
      logging:
        loglevel: info
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [jaeger, logging]
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, batch, resource]
          exporters: [prometheus, logging]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: annotateai
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.89.0
        ports:
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        - containerPort: 8889
          name: prometheus
        volumeMounts:
        - name: config-volume
          mountPath: /etc/otelcol-contrib
        args:
        - --config=/etc/otelcol-contrib/otel-collector.yaml
        resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 100m
            memory: 256Mi
      volumes:
      - name: config-volume
        configMap:
          name: otel-collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector-service
  namespace: annotateai
spec:
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318
  - name: prometheus
    port: 8889
    targetPort: 8889
---
# Application Performance Monitoring Dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-dashboard-config
  namespace: annotateai
data:
  apm-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "AnnotateAI - APM Dashboard",
        "tags": ["annotateai", "apm", "performance"],
        "timezone": "UTC",
        "panels": [
          {
            "id": 1,
            "title": "Request Traces",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(traces_received_total[5m])",
                "legendFormat": "Traces/sec"
              }
            ]
          },
          {
            "id": 2,
            "title": "Service Response Times",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service_name=\"annotateai-api-gateway\"}[5m]))",
                "legendFormat": "API Gateway p95"
              },
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service_name=\"annotateai-ai-service\"}[5m]))",
                "legendFormat": "AI Service p95"
              },
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service_name=\"annotateai-collaboration-service\"}[5m]))",
                "legendFormat": "Collaboration p95"
              }
            ]
          },
          {
            "id": 3,
            "title": "Error Rates by Service",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(http_requests_total{service_name=~\"annotateai.*\",code=~\"5..\"}[5m])",
                "legendFormat": "{{service_name}} errors"
              }
            ]
          },
          {
            "id": 4,
            "title": "Database Query Performance",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(db_query_duration_seconds_sum[5m]) / rate(db_query_duration_seconds_count[5m])",
                "legendFormat": "{{operation}} avg time"
              }
            ]
          },
          {
            "id": 5,
            "title": "Cache Hit Ratio",
            "type": "stat",
            "targets": [
              {
                "expr": "rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))",
                "legendFormat": "Hit Ratio"
              }
            ]
          },
          {
            "id": 6,
            "title": "Top Slow Operations",
            "type": "table",
            "targets": [
              {
                "expr": "topk(10, rate(operation_duration_seconds_sum[5m]) / rate(operation_duration_seconds_count[5m]))",
                "legendFormat": "{{operation}}"
              }
            ]
          },
          {
            "id": 7,
            "title": "Memory Usage by Service",
            "type": "graph",
            "targets": [
              {
                "expr": "process_resident_memory_bytes{service_name=~\"annotateai.*\"}",
                "legendFormat": "{{service_name}}"
              }
            ]
          },
          {
            "id": 8,
            "title": "GC Metrics",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(gc_duration_seconds_sum[5m])",
                "legendFormat": "{{service_name}} GC time"
              }
            ]
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "30s"
      }
    }
---
# Node.js APM instrumentation example
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-instrumentation
  namespace: annotateai
data:
  instrumentation.js: |
    const { NodeSDK } = require('@opentelemetry/sdk-node');
    const { Resource } = require('@opentelemetry/resources');
    const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');
    const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
    const { JaegerExporter } = require('@opentelemetry/exporter-jaeger');
    const { PrometheusExporter } = require('@opentelemetry/exporter-prometheus');
    
    // Custom instrumentations for AnnotateAI
    const sdk = new NodeSDK({
      resource: new Resource({
        [SemanticResourceAttributes.SERVICE_NAME]: process.env.SERVICE_NAME || 'annotateai-service',
        [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION || '1.0.0',
        [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV || 'production',
      }),
      traceExporter: new JaegerExporter({
        endpoint: 'http://jaeger-service:14268/api/traces',
      }),
      metricReader: new PrometheusExporter({
        port: 9090,
      }),
      instrumentations: [
        getNodeAutoInstrumentations({
          '@opentelemetry/instrumentation-fs': {
            enabled: false, // Disable file system instrumentation for performance
          },
          '@opentelemetry/instrumentation-http': {
            enabled: true,
            requestHook: (span, request) => {
              span.setAttributes({
                'http.request.header.user-agent': request.getHeader('user-agent'),
                'annotateai.user_id': request.getHeader('x-user-id'),
                'annotateai.project_id': request.getHeader('x-project-id'),
              });
            },
          },
          '@opentelemetry/instrumentation-express': {
            enabled: true,
          },
          '@opentelemetry/instrumentation-redis': {
            enabled: true,
          },
          '@opentelemetry/instrumentation-pg': {
            enabled: true,
          },
        }),
      ],
    });
    
    sdk.start();
    
    // Custom metrics for AnnotateAI
    const { MeterProvider } = require('@opentelemetry/sdk-metrics');
    const { Resource } = require('@opentelemetry/resources');
    const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');
    
    const meterProvider = new MeterProvider({
      resource: new Resource({
        [SemanticResourceAttributes.SERVICE_NAME]: process.env.SERVICE_NAME || 'annotateai-service',
      }),
    });
    
    const meter = meterProvider.getMeter('annotateai-metrics');
    
    // Business metrics
    const annotationCounter = meter.createCounter('annotateai_annotations_completed_total', {
      description: 'Total number of annotations completed',
    });
    
    const inferenceHistogram = meter.createHistogram('annotateai_ai_inference_duration_seconds', {
      description: 'AI inference duration in seconds',
    });
    
    const collaborationGauge = meter.createUpDownCounter('annotateai_active_collaboration_sessions', {
      description: 'Number of active collaboration sessions',
    });
    
    // Export metrics for use in application
    module.exports = {
      annotationCounter,
      inferenceHistogram,
      collaborationGauge,
    };
---
# Python APM instrumentation example
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-instrumentation-python
  namespace: annotateai
data:
  instrumentation.py: |
    import os
    from opentelemetry import trace, metrics
    from opentelemetry.sdk.trace import TracerProvider
    from opentelemetry.sdk.trace.export import BatchSpanProcessor
    from opentelemetry.sdk.metrics import MeterProvider
    from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
    from opentelemetry.exporter.jaeger.thrift import JaegerExporter
    from opentelemetry.exporter.prometheus import PrometheusMetricReader
    from opentelemetry.instrumentation.auto_instrumentation import sitecustomize
    from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
    from opentelemetry.instrumentation.requests import RequestsInstrumentor
    from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor
    from opentelemetry.instrumentation.redis import RedisInstrumentor
    from opentelemetry.sdk.resources import Resource
    from opentelemetry.semconv.resource import ResourceAttributes
    
    # Configure resource
    resource = Resource.create({
        ResourceAttributes.SERVICE_NAME: os.getenv('SERVICE_NAME', 'annotateai-ai-service'),
        ResourceAttributes.SERVICE_VERSION: os.getenv('SERVICE_VERSION', '1.0.0'),
        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: os.getenv('ENVIRONMENT', 'production'),
    })
    
    # Configure tracing
    trace.set_tracer_provider(TracerProvider(resource=resource))
    tracer = trace.get_tracer(__name__)
    
    jaeger_exporter = JaegerExporter(
        agent_host_name="jaeger-service",
        agent_port=6831,
    )
    
    span_processor = BatchSpanProcessor(jaeger_exporter)
    trace.get_tracer_provider().add_span_processor(span_processor)
    
    # Configure metrics
    prometheus_reader = PrometheusMetricReader(port=9090)
    metrics.set_meter_provider(MeterProvider(
        resource=resource,
        metric_readers=[prometheus_reader]
    ))
    
    meter = metrics.get_meter(__name__)
    
    # Custom metrics for AI services
    inference_counter = meter.create_counter(
        "annotateai_ai_inference_total",
        description="Total number of AI inferences",
    )
    
    inference_duration = meter.create_histogram(
        "annotateai_ai_inference_duration_seconds",
        description="AI inference duration in seconds",
    )
    
    model_accuracy = meter.create_gauge(
        "annotateai_model_accuracy",
        description="Current model accuracy",
    )
    
    # Auto-instrument common libraries
    FastAPIInstrumentor.instrument()
    RequestsInstrumentor.instrument()
    Psycopg2Instrumentor.instrument()
    RedisInstrumentor.instrument()
    
    # Custom instrumentation decorators
    def trace_ai_operation(operation_name):
        def decorator(func):
            def wrapper(*args, **kwargs):
                with tracer.start_as_current_span(operation_name) as span:
                    span.set_attribute("annotateai.operation", operation_name)
                    span.set_attribute("annotateai.model", kwargs.get('model', 'unknown'))
                    
                    start_time = time.time()
                    try:
                        result = func(*args, **kwargs)
                        span.set_attribute("annotateai.success", True)
                        return result
                    except Exception as e:
                        span.set_attribute("annotateai.success", False)
                        span.set_attribute("annotateai.error", str(e))
                        raise
                    finally:
                        duration = time.time() - start_time
                        inference_duration.record(duration, {"model": kwargs.get('model', 'unknown')})
                        inference_counter.add(1, {"model": kwargs.get('model', 'unknown')})
            return wrapper
        return decorator
    
    # Export for use in application
    __all__ = ['trace_ai_operation', 'tracer', 'meter', 'inference_counter', 'inference_duration', 'model_accuracy'] 