# aura â€“ Nextâ€‘Generation AIâ€‘Assisted Development Platform  
**Version 3 â€“ 18 July 2025**

---

## Executive Summary
|           | Detail |
|-----------|--------|
| **Service** | aura â€” an endâ€‘toâ€‘end AI development workspace that couples local privacyâ€‘first coding assistance with cloud superâ€‘context reasoning and G3Dâ€‘powered 3â€‘D visualisation. |
| **MVP Target** | A productionâ€‘ready hybrid IDE/CLI plugâ€‘in (VS Code + NeoVim) and standalone desktop client, shipping with offline quantised models and optional secure cloud burst. |
| **Revenue Potential** | \$40â€‘120 M ARR within 3 years. |
| **Investment Required** | \$3.0 M over 9 months (32 devs inc. 8 G3D specialists). |
| **Strategic Goal** | Become the deâ€‘facto "AI forge" that transforms the way teams plan, write, visualise and ship code. |

---

## 1 | Purpose & Vision
aura's mission is to **compress ideaâ€‘toâ€‘deployment time from days to minutes** by fusing:

* **AI Swarms** â€” orchestrated specialist agents for design, code, test, security & docs.  
* **Immersive 3â€‘D Visualisation** â€” G3D renders of architecture, callâ€‘graphs & live intent graphs.  
* **Hybrid AI** â€” lightningâ€‘fast local models for daily flow, superâ€‘context cloud brains for deep analysis.  
* **Enterpriseâ€‘grade Trust** â€” zeroâ€‘trust local inference, SOC 2 cloud, SBOM & signed artefacts.

---

## 2 | Strategic Insights
* **Developers want privacy & speed** â†’ local LLMs are exploding in adoption.  
* **Context windows still pinch** â†’ burstâ€‘toâ€‘cloud for 70 kâ€‘128 k tokens solves repoâ€‘scale reasoning.  
* **Tools â‰  UX** â†’ firstâ€‘wave AI plugâ€‘ins look boltedâ€‘on; devs crave *clarity* of what the agent does.  
* **Visual cognition beats text** â†’ codebase comprehension jumps when structure is spatially rendered.  
* **Open ecosystem wins** â†’ extensible marketplaces and OSSâ€‘friendly licences encourage viral growth.

---

## 3 | Market Opportunity
| Scope | Value |
|-------|-------|
| **TAM** (Dev tools) | \$26.5 B |
| **SAM** (AIâ€‘powered dev) | \$8.2 B |
| **SOM** (Hybrid + G3D niche) | \$1.2 B |

### Target Customers & ACVs
1. **Enterprise Teams** â€” \$100 kâ€‘1 M / yr  
2. **SaaS/Product Companies** â€” \$25 kâ€‘500 k / yr  
3. **DevOps & Platform Squads** â€” \$50 kâ€‘300 k / yr  
4. **Consultancies & SIs** â€” \$30 kâ€‘200 k / yr  
5. **Gov & Defence** â€” \$75 kâ€‘750 k / yr

## 3.1 User Personas & JTBD

| Persona | Pain Point | "Jobâ€‘toâ€‘Beâ€‘Done" | Key Success Metric |
|---------|------------|-----------------|--------------------|
| **Indie Hacker** | Contextâ€‘switch fatigue, infra cost | "Ship a sideâ€‘project tonight without installing Docker." | Prototype deployed in < 4 h |
| **Senior Backend Engineer** | Legacy refactor risk | "Confidently migrate a monolith to services without breaking prod." | < 1 % rollback rate |
| **Team Lead / EM** | Review bottlenecks | "Keep PR queue < 24 h while mentoring juniors." | Review SLA met 95 % |
| **Security Engineer** | Unknown code paths | "Surface Ossâ€‘vulnerable deps & secret leaks continuously." | Zero critical CVEs at release |
| **DevRel Advocate** | Community engagement | "Author demos & tutorials that wow in < 1 day." | GitHub stars & Discord joins |

### 3.2 Core Workflow Scenarios

1. **Greenfield Wizard** â€“ "Generate a Rust microservice template with CI, tests, Terraform."  
2. **Surgical Bug Fix** â€“ Point AI at failing test; agent proposes patch, autoâ€‘opens ghostâ€‘branch PR.  
3. **Architectural Refactor** â€“ Visual Intentâ€‘Graph highlights coupling; AI splits modules, updates Docs.  
4. **Security Sweep** â€“ Securityâ€‘Swarm agent runs SAST+SBOM, inserts fixes & compliance badges.  
5. **XR Code Walkthrough** â€“ VR session: lead walks new hire through 3â€‘D callâ€‘graph, AI answers Q&A.

---

## 4 | Competitive Analysis

| Competitor | Differentiator | Model Strat. | Collab | Visualâ€‘isation | Arch/Plan | UX Mode | Extens. | Enterprise | Security | Licence |
|------------|----------------|--------------|--------|----------------|-----------|---------|---------|------------|----------|---------|
| **Cursor** | AIâ€‘native editor | Cloud | Basic | Limited | Medium | Custom IDE | Mod. | Growing | Mod. | Proprietary |
| **GitHub Copilot** | Inline completion | Cloud | Basic | None | Low | IDE plugâ€‘in | Low | Strong via GitHub | Mod. | Proprietary |
| **Windsurf** | "Vibeâ€‘coding" web IDE | Cloud | Mod. | Basic | Low | Web IDE | Mod. | Strong | Mod. | Proprietary |
| **Lovable** | NLâ†’App builder | Cloud | Good | Low | Medium | Chat/web | Low | Growing | Mod. | Proprietary |
| **Replit** | Browser IDE + agent | Cloud | Excellent | Low | Medium | Web IDE | Mod. | Mod. | Mod. | Proprietary |
| **Claude Code** | CLI agentic | Hybrid | Low | None | High | CLI | High | Mod. | Strong | Proprietary |
| **Cline** | OSS plan/act agent | Local | Mod. | None | Medium | VS Code ext | High | Low | Good | OSS |
| **Roo** | Multiâ€‘agent OSS | Local | Good | Med. | High | VS Code ext | High | Low | Good | OSS |
| **Kilo Code** | Modular OSS | Local | Excellent | Med. | High | VS Code ext | Very High | Mod. | Excellent | OSS |
| **aura** | **3â€‘D G3D viz + AI swarms + hybrid local/cloud** | **Hybrid** | **Excellent** | **Adv. 3â€‘D** | **Intentâ€‘Graph** | IDE + Desktop | **Marketplace** | **Enterprise suite** | **Zeroâ€‘trust** | **Hybrid (commercial core + open SDK)** |

**Why aura Wins**

* **Visual Clarity** â€“ Only platform with *realâ€‘time 3â€‘D* architecture & dependency renderings.  
* **Hybrid Speed + Power** â€“ 50 ms local completions *and* GPTâ€‘4â€‘level deep refactors.  
* **Intent Graph** â€“ Keeps requirements â‡† code in sync, so agents never "hallucinate scope."  
* **Open Extensibility** â€“ Plugin SDK, prompt packs, model mesh; no vendor lockâ€‘in.  
* **Enterpriseâ€‘first Trust** â€“ Onâ€‘prem option, signed artefacts & full audit trail.

---

## 5 | Unique Value Proposition  
> *"Forge ideas into productionâ€‘ready code with the clarity of 3â€‘D and the power of a senior AI swarm â€” online or offline."*

---

## 6 | MVP Feature Set

**Core Forge Components**
- Desktop & VS Code plugâ€‘ins
- Quantised 6B local model integration
- 3â€‘D miniâ€‘map visualization
- Ghostâ€‘branch PR workflow

**Collaboration & XR Features**
- Intentâ€‘Graph panel
- AI swarm personas
- VR/AR code walkthrough capabilities
- Marketplace alpha

**Enterprise Readiness**
- SSO and RBAC implementation
- SOCâ€‘2 compliance pipeline
- Cloud burst to 70B model
- SBOM export functionality
- Private VPC deployment options

### 6.1 MVP Exit / Acceptance Criteria

* **Local Model Performance** < 60 ms p95 for local completions on M4 Max with 7-model strategy.  
* **Model Accuracy** â‰¥ 92.9% pass@1 on HumanEval (Qwen3-Coder), â‰¥ 90% on internal EvalPlus suite.
* **Context System** â‰¤ 50ms context retrieval, â‰¤ 300MB RAM footprint, â‰¥ 70% hit rate.
* **Storage Management** Handles 110GB+ model downloads with intelligent cleanup.
* **Model Routing** Intelligent task classification across 7 local + 2 cloud models.
* **Zeroâ€‘Trust** mode passes thirdâ€‘party pentest (full offline operation).  
* **3â€‘D Viz** renders 1 Mâ€‘LOC repo at â‰¥ 30 FPS on RTX 3070.  
* **Marketplace** publishes/installs plugâ€‘ins via signed manifest.  
* **CI Integration** supports GitHub Actions & GitLab CI templates.

---

### 6.2 | Killer Workflow â€“ Visual-Guided Refactor (VGR)

|               | Detail |
|---------------|--------|
| **Goal** | Slash pull-request merge time by â‰¥ 70 % for complex refactors through an AI-assisted, 3-D visual workflow. |
| **Primary Persona** | Senior Backend Engineer refactoring legacy modules while maintaining SLA. |
| **Key Metric** | **Time-to-Merge (TTM)** for flagged PRs â‰¤ 4 h (baseline 16 h). |

#### User Stories
1. **Identify Impact**  
   *As a developer* I can drag-and-drop a file/function in the 3-D call-graph to instantly visualise all upstream/downstream dependencies so that I know the blast radius before editing.

2. **Generate Safe Patch Set**  
   *As a developer* I can click **â€œRefactor with AIâ€** and receive a ghost-branch containing:  
   - updated imports & file moves  
   - auto-updated tests  
   - migration notes in `CHANGELOG.md`.

3. **Confidence Gate**  
   *As a reviewer* I see an AI-generated diff summary with risk score, impacted tests, and green CI badge so I can approve in one pass.

#### Functional Requirements
| ID | Requirement |
|----|-------------|
| VGR-F-01 | Real-time G3D scene highlighting affected nodes (< 100 ms on 1 M-LOC repo). |
| VGR-F-02 | AI swarm invokes **Planner â†’ Transformer â†’ Tester â†’ Doc-Bot** sequence. |
| VGR-F-03 | Generates *ghost-branch* PR with linked **Intent-Graph** delta. |
| VGR-F-04 | Intelligent model routing: Qwen3-Coder for previews, Phi-4-mini for agentic tasks, cloud burst (DeepSeek R1) for complex refactors > 4k context. |
| VGR-F-05 | Provides rollback button restoring pre-refactor state. |

#### Non-Functional Requirements
* **Latency**: Visualization update < 150 ms p95.  
* **Accuracy**: 95 % unchanged test pass rate post-merge.  
* **Security**: Operates fully in **Zero-Trust** mode (no outbound net).  

#### Acceptance Criteria
1. Triggering VGR on the sample 50-file legacy module finishes < 3 min incl. tests.  
2. Reviewer approves 80 % of VGR PRs without manual code comments in pilot org.  
3. Post-merge telemetry shows no production regression in 30-day canary window.

#### Risks & Mitigations
| Risk | Mitigation |
|------|------------|
| Scene overload on huge monorepos | Chunked lazy-loading + LOD culling in G3D renderer |
| AI patch hallucination | Two-stage refine: cloud output diff-checks local draft; run unit & type checks before PR |

#### Dependencies
* **Model Infrastructure**: Phase 1.5 model download system for 7 local models (ModelLoader.ts, ModelRouter.ts).
* **Context Engine**: Phase 1.6 Dynamic Context Persistence (FileWatcher.ts, ASTIndexer.ts, VectorDB.ts, SemanticStore.ts).
* **Intelligent Routing**: Model Router with task classification across 9 model endpoints.
* **G3D Rendering**: Incremental scene-graph diffing in G3D (Engine ticket G3D-1201).  
* **AI Swarm**: Swarm orchestrator plug-in API (Core ticket CORE-412).  
* **CI Integration**: In-repo CI templates (DevOps ticket DEV-88).

### 6.3 | Goâ€‘toâ€‘Market Funnel â€“ Extension âžœ Desktop âžœ CLI

| Stage | Surface | Primary CTA / Goal | Key Metrics |
|-------|---------|--------------------|-------------|
| **Discover / Try** | VS Code & NeoVim extension (`ms-aura.vsix`) | Give developers a < 5 MB â€œtasteâ€ of AI swarm and 3â€‘D miniâ€‘map without leaving their editor. | Conversion to desktop installer, weekly active installs |
| **Adopt / Delight** | **Standalone desktop client** (Electron/Tauri) â€“ *hero experience* | Unlock local 6â€‘8 GB models, 120 Hz WebGPU, XR walkthroughs, and offline mode. | Daily active users, NPS, modelâ€‘latency p95 |
| **Automate / Integrate** | `aura` **CLI** (headless) | Drive CI hooks, Git aliases, and terminalâ€‘editor workflows. | CI minutes saved, script invocations per repo |

> **Launch order:** extension ships first (funnel top), desktop GA follows within 4 weeks, CLI is bundled with desktop but documented for standalone use.

### 6.4 | Engineering Effort Split (MVP 9â€‘month budget)

| Workâ€‘stream | Effort Share | Rationale |
|-------------|-------------|-----------|
| Core AI coding & refactor | **45 %** | Model orchestration, swarm personas, ghostâ€‘branch PRs |
| 3â€‘D visualisation (flat) | **25 %** | G3D renderer, callâ€‘graph & intentâ€‘graph |
| Collaboration & marketplace | **15 %** | CRDT presence, plugâ€‘in SDK, inâ€‘app store |
| **XR / VR extras** | **10 %** | VR camera, avatar sync, Quest/Vision Pro shell |
| Compliance & ops plumbing | **5 %** | SOCâ€‘2 pipeline, updater, BYOâ€‘key router guards |

*XR is capped at ~10 % to keep focus on dayâ€‘toâ€‘day developer value while still enabling wowâ€‘factor demos.*

## 7 | Technical Architecture

### 7.1 Highâ€‘Level Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOCAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€ CLOUD APIs â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚    â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ IDE â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ Model Router â”‚â”€â”€â”€â”€â”€â–¶â”‚ 7 Local Models  â”‚    â”‚    â”‚  â”‚ Kimi K2 API     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜       â”‚              â”‚      â”‚ â€¢ Qwen3-Coder   â”‚â—€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â–¶â”‚ (Agentic)       â”‚    â”‚
â”‚                â”‚ Task         â”‚      â”‚ â€¢ Phi-4-mini    â”‚    â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”       â”‚ Classifier   â”‚      â”‚ â€¢ Gemma 3       â”‚    â”‚    â”‚             â”‚           â”‚
â”‚  â”‚ CLI â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚              â”‚      â”‚ â€¢ Llama 3.3-70B â”‚    â”‚    â”‚             â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜       â”‚              â”‚      â”‚ â€¢ Starcoder2    â”‚    â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                â”‚              â”‚      â”‚ â€¢ DeepSeek-V2   â”‚    â”‚    â”‚  â”‚ DeepSeek R1 API â”‚    â”‚
â”‚                â”‚              â”‚      â”‚ â€¢ Mistral Dev   â”‚    â”‚    â”‚  â”‚ (Reasoning)     â”‚    â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â”‚                        â”‚           â”‚    â”‚             â”‚           â”‚
â”‚                       â–¼                        â–¼           â”‚    â”‚             â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Dynamic Context Engine  â”‚     â”‚    G3D Renderer      â”‚ â”‚    â”‚  â”‚ BYO-Key APIs    â”‚    â”‚
â”‚  â”‚ â€¢ FileWatcher          â”‚     â”‚                       â”‚â”€â”¼â”€â”€â”€â”€â”¼â”€â–¶â”‚ â€¢ OpenAI        â”‚    â”‚
â”‚  â”‚ â€¢ ASTIndexer           â”‚     â”‚                       â”‚ â”‚    â”‚  â”‚ â€¢ Anthropic     â”‚    â”‚
â”‚  â”‚ â€¢ VectorDB             â”‚     â”‚                       â”‚ â”‚    â”‚  â”‚ â€¢ Google        â”‚    â”‚
â”‚  â”‚ â€¢ SemanticStore        â”‚     â”‚                       â”‚ â”‚    â”‚  â”‚ â€¢ xAI           â”‚    â”‚
â”‚  â”‚ â€¢ ContextPlanner       â”‚     â”‚                       â”‚ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”‚ â€¢ Retriever            â”‚     â”‚                       â”‚ â”‚    â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                       â–²                        â”‚           â”‚    â”‚  â”‚ Encrypted Logs   â”‚    â”‚
â”‚                       â”‚                        â”‚           â”‚    â”‚  â”‚ (Telemetry)     â”‚    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚           â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”‚    Intent DB           â”‚                   â”‚           â”‚    â”‚             â–²           â”‚
â”‚  â”‚                        â”‚                   â”‚           â”‚    â”‚             â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€statsâ”€â”€â”€â”˜           â”‚
â”‚                                                             â”‚    â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                                                                   â”‚  â”‚ Plugin Market   â”‚    â”‚
                                                                   â”‚  â”‚                 â”‚    â”‚
                                                                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                                                                   â”‚             â–²           â”‚
                                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                                 â”‚
                                                                             plugins
                                                                                 â”‚
                                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                                                                 â”Œâ”€â”€â”€â”€â”€â”
                                                                 â”‚ IDE â”‚
                                                                 â””â”€â”€â”€â”€â”€â”˜
```

### 7.2 Frontâ€‘end Stack

* **React 18 + TypeScript**
* **Monaco (G3Dâ€‘enhanced)** editor
* **WebGPU** G3D renderer
* **Redux Toolkit + Zustand** state
* **WebRTC + Liveblocks** for multiâ€‘user + XR sessions

### 7.3 Backâ€‘end Stack

* Node.js (REST) + **FastAPI** (AI microâ€‘services)
* PostgreSQL (metadata) & **DuckDB** (telemetry)
* **Gitâ€‘FS** storage with delta packing
* Docker + **Kubernetes** (Helm)

### 7.4 **Local & Hybrid AI Model Strategy**

**ðŸŽ¯ Core Model Selection (July 2025)**

We use **7 local model families + 2 cloud APIs** optimized for different scenarios:

**ðŸ’» Local Models (Downloaded & Run Locally):**

1. **Qwen3-Coder** (PRIMARY LOCAL) - Default coding assistant
   - Primary workhorse for code completion, generation, and refactoring
   - 4 size variants: 4B (ultra-light), 8B (balanced), 14B (premium), 32B (workstation)
   - Ships with installer, runs offline - 92.9% HumanEval performance

2. **Phi-4-mini** (LOCAL AGENTIC) - Local agentic workflows
   - 3.8B parameters optimized for function calling and tool use
   - Primary local model for autonomous coding tasks
   - Lightweight alternative to cloud agentic APIs

3. **Gemma 3** (LOCAL MULTIMODAL) - Advanced capabilities
   - 3 variants: 4B, 12B, 27B parameters
   - 12B variant includes multimodal capabilities for visual tasks
   - Strong reasoning performance with Google's training

4. **Mistral Devstral Small** (LOCAL CONTEXT) - Extended context
   - 24B parameters with 256K context windows
   - Strong on real-world coding benchmarks (87.5% HumanEval)
   - Apache 2.0 license for commercial use

5. **Llama 3.3-70B** (LOCAL POWER) - Heavy local reasoning
   - 70B parameters for workstation users with 48GB+ VRAM
   - Meta's latest coding model with superior reasoning
   - Alternative to cloud burst for privacy-critical scenarios

6. **Starcoder2-15B** (LOCAL POLYGLOT) - Multi-language specialist
   - 15B parameters supporting 600+ programming languages
   - Optimized for polyglot programming and code translation
   - Excellent for diverse tech stacks

7. **DeepSeek-Coder V2 Lite** (LOCAL EFFICIENT) - Fast inference
   - 16B parameters with MoE (Mixture of Experts) architecture
   - Optimized for fast inference with high efficiency
   - Alternative coding model with competitive performance

**â˜ï¸ Cloud APIs (aura Managed):**

8. **Kimi K2** (CLOUD AGENTIC) - Advanced agentic workflows
   - 1T parameter MoE model (32B active) optimized for tool use
   - 53.7% LiveCodeBench, 65.8% SWE-bench performance
   - Cost-effective at $0.60/$2.50 per M tokens

9. **DeepSeek R1** (CLOUD REASONING) - Complex reasoning
   - 671B parameter model with advanced reasoning capabilities
   - Full codebase understanding and large refactors
   - Cloud burst when context >4k tokens or complexity high

**ðŸŽ¯ Summary - Complete Model Matrix:**

| Model | Purpose | Deployment | Size | Why We Chose It |
|-------|---------|------------|------|-----------------|
| **Qwen3-Coder** | Primary coding | Local (auto-install) | 4B/8B/14B/32B | Best HumanEval scores, 4 size options |
| **Phi-4-mini** | Local agentic | Local (optional) | 3.8B | Function calling, lightweight agentic |
| **Gemma 3** | Multimodal tasks | Local (optional) | 4B/12B/27B | Google's multimodal capabilities |
| **Mistral Devstral** | Long context | Local (optional) | 24B | 256K context, strong benchmarks |
| **Llama 3.3-70B** | Heavy reasoning | Local (optional) | 70B | Meta's latest, workstation power |
| **Starcoder2** | Polyglot coding | Local (optional) | 15B | 600+ languages, code translation |
| **DeepSeek-Coder V2** | Fast inference | Local (optional) | 16B MoE | Efficient MoE architecture |
| **Kimi K2** | Cloud agentic | Cloud API | 1T (32B active) | Superior autonomous workflows |
| **DeepSeek R1** | Cloud reasoning | Cloud API | 671B | Advanced reasoning capabilities |

**ðŸ”‘ BYO-Key APIs (User Provides Keys):**
- **OpenAI**: GPT-4.1, o3-mini for premium completions
- **Anthropic**: Claude 4 Opus/Sonnet for best reasoning  
- **Google**: Gemini 2.5 Pro with 2M context window
- **xAI**: Grok 4 for real-time information
- **Meta**: Llama 4 70B via cloud providers
- **Custom**: Any OpenAI-compatible API endpoint

**Orchestration Logic**

1. ***Code completion*** â†’ Qwen3-Coder (local, size based on hardware) for suggestions < 60 ms.
2. ***Local agentic tasks*** â†’ Phi-4-mini (local) for function calling and autonomous workflows.
3. ***Cloud agentic tasks*** â†’ Kimi K2 (cloud API) for complex multi-step workflows.
4. ***Complex reasoning*** â†’ DeepSeek R1 (cloud) when context >4k tokens or architectural decisions.
5. ***Multimodal tasks*** â†’ Gemma 3-12B (local) or cloud APIs for visual/diagram processing.
6. ***Polyglot programming*** â†’ Starcoder2-15B (local) for 600+ language support.
7. ***Long context (256K)*** â†’ Mistral Devstral (local) or Gemini 2.5 Pro (cloud).
8. ***Heavy local reasoning*** â†’ Llama 3.3-70B (local, workstation only) for privacy-critical scenarios.
9. ***Fast inference*** â†’ DeepSeek-Coder V2 Lite (local) for efficient MoE processing.

---

### 7.4.1 Model Download & Installation Strategy

**ðŸ“¦ What Gets Downloaded and When?**

| Model | When | How | Storage | Use Case |
|-------|------|-----|---------|----------|
| **Qwen3-Coder 14B** | During install | Automatic (required) | 7.8 GB | Default coding assistant |
| **Qwen3-Coder 8B** | On-demand | Settings â†’ Models â†’ Install | 4.5 GB | Lower-spec machines |
| **Qwen3-Coder 4B** | On-demand | Settings â†’ Models â†’ Install | 2.2 GB | Ultra-light laptops |
| **Qwen3-Coder 32B** | On-demand | Settings â†’ Models â†’ Install | 18 GB | Workstation performance |
| **Phi-4-mini** | Auto-install | Bundled with installer | 2.8 GB | Local agentic workflows |
| **Gemma 3-4B** | On-demand | Settings â†’ Models â†’ Install | 3.2 GB | Lightweight multimodal |
| **Gemma 3-12B** | On-demand | Settings â†’ Models â†’ Install | 8.1 GB | Advanced multimodal |
| **Gemma 3-27B** | On-demand | Settings â†’ Models â†’ Install | 16.5 GB | High-end multimodal |
| **Mistral Devstral Small** | On-demand | Settings â†’ Models â†’ Install | 14.2 GB | Long context (256K) |
| **Llama 3.3-70B** | On-demand | Settings â†’ Models â†’ Install | 35 GB | Heavy local reasoning |
| **Starcoder2-15B** | On-demand | Settings â†’ Models â†’ Install | 8.7 GB | Polyglot programming |
| **DeepSeek-Coder V2 Lite** | On-demand | Settings â†’ Models â†’ Install | 9.6 GB | Fast MoE inference |
| **Kimi K2** | Never | Cloud API only | 0 GB | Cloud agentic workflows |
| **DeepSeek R1** | Never | Cloud API only | 0 GB | Complex reasoning |

**ðŸ”„ Download Process:**
- Models fetched from **Hugging Face** using `llama.cpp` compatible GGUF format
- **Chunked downloads** with resume capability (no restart on connection loss)
- **SHA-256 verification** ensures model integrity
- **Background downloads** - keep coding while models fetch
- **Delta updates** - only download changed layers on model updates

**ðŸ’¾ Typical Storage Requirements:**

| User Profile | Models | Total Disk | GPU VRAM | Use Case |
|--------------|---------|------------|----------|----------|
| **Minimal** | Qwen3-4B + Phi-4-mini | 5 GB | 4 GB | Ultra-light setup |
| **Standard** | Qwen3-14B + Phi-4-mini | 10.6 GB | 8 GB | Default recommended setup |
| **Power User** | Qwen3-14B + Phi-4-mini + Gemma-12B + Starcoder2 | 27.2 GB | 16 GB | Multi-task capabilities |
| **Enthusiast** | Qwen3-32B + Multiple variants | 60 GB | 24 GB | High-performance local |
| **Workstation** | Llama-70B + Full suite | 110+ GB | 48+ GB | Maximum local capabilities |
| **Corporate** | Qwen3-4B + Cloud APIs | 5 GB | 4 GB | Light local + cloud burst |

**ðŸš€ Model Selection by Hardware:**

| Your GPU | Recommended Setup | Experience |
|----------|-------------------|------------|
| **RTX 3060 (12GB)** | Qwen3-14B + Phi-4-mini | Smooth completions + local agentic |
| **RTX 3090 (24GB)** | Qwen3-14B + Phi-4-mini + Gemma-12B + Starcoder2 | Multi-model capabilities |
| **RTX 4070 (12GB)** | Qwen3-14B + Phi-4-mini + Mistral Devstral | Balanced performance |
| **RTX 4090 (24GB)** | Qwen3-32B + Multiple models | High-end local inference |
| **H100 (80GB)** | Llama-70B + Full suite | Maximum local power |
| **M2/M3 Mac (16GB)** | Qwen3-8B + Phi-4-mini + Cloud | Optimized for Apple Silicon |
| **Laptop (8GB)** | Qwen3-4B + Cloud APIs | Ultra-light local assist |

**ðŸ“Š When Each Model Gets Used:**

```
Input â†’ Task Classification â†’ Model Selection
â”œâ”€ Code completion â†’ Qwen3-Coder (local, size based on hardware)
â”œâ”€ Local agentic task â†’ Phi-4-mini (local function calling)
â”œâ”€ Cloud agentic task â†’ Kimi K2 (cloud API)
â”œâ”€ Complex reasoning â†’ DeepSeek R1 (cloud, >4K tokens)
â”œâ”€ Multimodal task â†’ Gemma 3-12B (local) or cloud APIs
â”œâ”€ Polyglot programming â†’ Starcoder2-15B (local)
â”œâ”€ Long context (256K) â†’ Mistral Devstral (local) or Gemini 2.5 Pro (cloud)
â”œâ”€ Heavy reasoning (privacy) â†’ Llama 3.3-70B (local, workstation)
â””â”€ Fast MoE inference â†’ DeepSeek-Coder V2 Lite (local)
```

---

## 7.5 | Instrumentation & Experimentation

* **Event Schema** â€” PageView, SuggestionAccepted, CloudBurst, VizFrameRate.  
* **Privacy** â€” Events anonymised w/ differential privacy; optâ€‘out toggle.  
* **A/B Harness** â€” Flagâ€‘based rollouts (e.g., new prompt strategy) via OpenFeature SDK.  
* **Northâ€‘Star Metric** â€” "Timeâ€‘toâ€‘Merge" from first code edit to PR merge.

---

## 7.6 | Model Evaluation

| Benchmark | Metric | Target | Cadence |
|-----------|--------|--------|---------|
| **HumanEval+ 164** | pass@1 | â‰¥ 80 % local / â‰¥ 90 % cloud | Nightly |
| **SWEâ€‘bench** | solved % | â‰¥ 35 % cloud | Weekly |
| **CSâ€‘Eval (CompSci Qâ€‘A)** | accuracy | â‰¥ 75 % local | Monthly |

Openâ€‘source leaderboard scripts will run inside GitHub Actions, posting badged results to README.

---

## 7.7 | Compliance Matrix

| Domain | Requirement | Design Measure |
|--------|-------------|----------------|
| **EU AI Act** | Risk management, transparency | Model cards + audit log export |
| **SOC 2 Type II** | Security, Availability | Changeâ€‘control + disasterâ€‘recovery runbooks |
| **FedRAMP Low** | Gov pilot | AWS GovCloud template, FIPS TLS |

---

## 7.8 | Threat Model Highlights

| Category | Example Threat | Mitigation |
|----------|----------------|------------|
| **Spoofing** | Malicious plugâ€‘in masquerades | Signed marketplace, key pinning |
| **Tampering** | Model weight injection | SHAâ€‘256 integrity + secure boot |
| **Repudiation** | Dev denies code change | Immutable ghostâ€‘branch history |
| **Information Disclosure** | Cloud logging leak | Perâ€‘tenant KMS keys |
| **DoS** | Prompt bombs | Rate & tokenâ€‘limiters per org |
| **Elevation** | Privâ€‘escalation in agent shell | Seccomp jailed subprocess |

---

## 7.9 | Openâ€‘Source Strategy

* **Core Closedâ€‘Source Modules** â€” Swarm orchestrator, 3â€‘D renderer shaders.  
* **Apacheâ€‘2.0 SDK** â€” Plugâ€‘in API, model mesh adapters.  
* **Dualâ€‘Licensing** â€” GPL fusion detection via FOSSology; SPDX manifests autoâ€‘generated.

---

## 7.10 | Marketplace Governance

1. **Submission Review** â€” Static scan, signature, automated tests (â‰¤ 2 h SLA).  
2. **Revenue Split** â€” 80 % creator / 20 % aura; payouts via Stripe Connect.  
3. **Takedown Process** â€” 24â€‘hour security/DMCA response window.  
4. **Quality Ranking** â€” TrustScore combines install volume + crash telemetry.

---

## 7.11 | Performance & Scalability Targets

* **Local Memory Footprint** â‰¤ 1.2Ã— repo size.  
* **Cloud Inference TPS** â€” 20 req/s per A100 with batching.  
* **Horizontal Scale** â€” K8s HPA on GPU utilisation > 60 %.  
* **SLOs** â€” 99.5 % â‰¤ 500 ms P90 /suggest API.

---

## 7.12 | Accessibility / Internationalisation

* 3â€‘D colour palettes WCAG AA contrast.  
* Screenâ€‘readerâ€‘friendly diff views (ARIA live regions).  
* Prompt packs localised (EN, ZH, ES, PT, HI) using ICU message format.

---

## 7.13 | Developer Environment

* **Oneâ€‘Command Dev** â€” `make dev` spins Docker compose (Postgres, Minio, Superset).  
* **Git Hooks** â€” Preâ€‘commit Black & ESLint; preâ€‘push model eval smoke.  
* **Seed Scripts** â€” Demo repo + synthetic bug tickets for sandbox demos.

---

## 7.5 | Dynamic Context Persistence System ðŸ†•

**ðŸŽ¯ Goal:** Persistent, real-time code context so local LLMs never "forget" the project.

### Memory Foundation (Phase 1.6)

| Component | Purpose | Performance Target |
|-----------|---------|-------------------|
| **FileWatcher.ts** | Cross-platform FS watchers with debounced events | < 10ms event processing |
| **ASTIndexer.ts** | Incremental Tree-sitter diff parser â†’ symbol & embedding queue | < 200ms incremental updates |
| **VectorDB.ts** | Local Qdrant/Faiss wrapper for K-NN search | < 50ms vector retrieval |
| **SemanticStore.ts** | Multi-tier store (hot in-mem, warm SQLite, cold Parquet) | < 100ms semantic queries |

### Context Engine GA (Phase 2.0)

| Component | Purpose | Integration |
|-----------|---------|-------------|
| **ContextPlanner.ts** | Classify IDE/CLI events â†’ intent labels | Hooks into editor events |
| **Retriever.ts** | Hybrid relevance ranker (BM25 + cosine + recency) | Assembles context for LLM calls |
| **PromptAssembler.ts** | Builds final prompt with compression & safety margin | Injects into model requests |
| **MemoryPanel.tsx** | Shows "live context" chunks, tokens, pin/unpin UI | Real-time context visualization |
| **MemorySettings.tsx** | Disk quota, expiry, privacy toggles | User control over memory system |

### Memory Feedback Loop (Phase 3)

**RelevanceLearner.ts** - Online learning from user feedback:
- ðŸ‘/ðŸ‘Ž buttons update chunk relevance scores
- Merge success metrics improve context selection
- Multi-armed bandit optimization of ranking weights

### Performance Specifications

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Context Retrieval Latency** | â‰¤ 50ms p95 | Planner + Retriever + Assembler pipeline |
| **Extra RAM Footprint** | â‰¤ 300MB | At 100k LOC indexed with hot cache |
| **Disk Overhead** | â‰¤ 1% | Cold + warm tiers vs raw repo size |
| **Context Hit Rate** | â‰¥ 70% | Relevant context chunks retrieved |
| **LLM Suggestion Validity** | +15pp | Improvement vs baseline without context |

### Key Innovation Benefits

1. **Never Loses Context** - Local LLMs maintain project awareness across sessions
2. **Intelligent Relevance** - ML-powered ranking learns from developer feedback  
3. **Privacy-First** - All context processing happens locally
4. **Real-Time Updates** - Sub-250ms incremental indexing on file saves
5. **Multi-Tier Storage** - Hot/warm/cold optimization for large codebases

---

## 8 | Strategic Modelâ€‘Vendor Integration Plan

> Goal: deliver "bestâ€‘available" intelligence on Day 1 while protecting aura from singleâ€‘vendor lockâ€‘in or sudden price spikes.

### 1. Multiâ€‘Vendor Abstraction ("Model Mesh")

| Adapter | Primary Strength | Typical Useâ€‘Case | Current SLA* |
|---------|------------------|------------------|--------------|
| **OpenAI (GPTâ€‘4.1/o3)** | Highest raw accuracy & advanced reasoning | Complex refactors, doc generation | 99.9 % / 40 k TPM |
| **Anthropic Claude 4 Sonnet/Haiku** | Very long context (200 k) & safety, thinking variants | Large repo Q&A, policy checks | 99.9 % / 40 k TPM |
| **Google Gemini 2.5 Pro** | Multimodal + 1M context + inline execution | Architecture sketches, UML autoâ€‘draw | 99.95 % / 60 k TPM |
| **xAI Grok 4** | Fast reasoning, 256k context, OSâ€‘friendly terms | Exploratory brainstorming, bug hunts | 99.8 % / 30 k TPM |
| **DeepSeek R1 671B (selfâ€‘host)** | SOTA open code model with reasoning | Privacyâ€‘critical cloud burst | N/A (selfâ€‘ops) |

\*SLAs based on vendor documentation July 2025.

An internal **LLM Router** ranks adapters by *Cost Ã— Latency Ã— Quality* for each request class ("completion", "analysis", "vision", etc.), with user/org policy overrides.

### 2. BYOâ€‘Key & Bringâ€‘Yourâ€‘Ownâ€‘Model

* **Pro & Enterprise users** can plug their own OpenAI / Claude / Gemini keys.  
* **Onâ€‘Prem option** allows customers to swap in a private or fineâ€‘tuned model behind our gRPC shim (e.g., IBMâ€‘Watsonâ€‘Code, local Qwenâ€‘72B).

#### 8.2.1 Commercial Guardâ€‘Rails for BYO & Onâ€‘Prem

| Lever | What it does | Revenue / Margin Impact |
|-------|--------------|-------------------------|
| **Token ceilings** | Dev/Team tiers include daily cloud tokens; overages trigger payâ€‘asâ€‘youâ€‘go blocks or BYO prompt. | Protects gross margin on highâ€‘usage accounts |
| **Smartâ€‘Router+ analytics addâ€‘on** | \$10â€‘20/seat for costâ€‘optimisation dashboards, cache hitâ€‘rates, vendor SLA alerts. | New ARR line, upsell path |
| **Premium swarm personas** | Securityâ€‘Sweep & SBOM signer run only on auraâ€‘hardened models. | Ensures some paid tokens even for BYO orgs |
| **Runtime licence for onâ€‘prem GPUs** | \$5â€¯k/GPU/year for DeepSeek R1â€‘671B container key. | Captures value from selfâ€‘hosted clusters |
| **Support SLAs** | 9Ã—5 email (included), 24Ã—7 pager (EnterpriseÂ Plus). | Service revenue & enterprise confidence |

These guardâ€‘rails let us embrace BYO without capping upside, while keeping COGS predictable.

### 3. Negotiation & Partnership Tracks

| Vendor | Current Status | Q3â€‘2025 Objectives |
|--------|----------------|--------------------|
| **OpenAI** | Tierâ€‘1 Build Partner; 3 M free tokens/mo | Secure *pilotâ€‘partner* pricing; GPT-4.1/o3 optimization |
| **Anthropic** | Devâ€‘Program seat | Access Claude 4 Sonnet/Opus fineâ€‘tune, volume discount >= 100 M tokens/month |
| **Google** | Cloud credits via AI Startup Program | Gemini 2.5 Pro integration for advanced reasoning tier |
| **xAI** | In discussion | GPU coâ€‘location deal for Grokâ€‘Edge inferencing |
| **AWS Bedrock** | Onâ€‘hold | Evaluate when CodeWhispererâ€‘Next public |

### 4. Data Governance & Compliance

* **No vendor retains customer code** â†’ `X-Data-Policy: opt-out-training` header where supported.  
* **Doubleâ€‘encryption in transit** â€” mTLS to vendor endpoint plus payload encryption for highlyâ€‘regulated clients.  
* **Regional routing** â€” EU traffic pinned to EU model endpoints (GDPR / EUâ€‘AIâ€‘Act).  

### 5. Cost & Failâ€‘over Logic

1. **Primary** route to cheapest model meeting *Quality â‰¥ Target* (e.g., Claude Haiku for largeâ€‘context doc chat).  
2. **Fallback A** â€” next provider in ranking if latency > 1 s or HTTP 5xx.  
3. **Fallback B** â€” selfâ€‘hosted DeepSeek R1 (autoâ€‘scale GPU pods) when external vendors down.  
4. **Budget Guardrails** â€” orgâ€‘level token ceilings + Slackâ€‘alert when 80 % reached.

### 6. API Surface (Developer Facing)

```yaml
POST /forge/ai/invoke
{
  "task": "code.review",
  "modelPref": ["local", "openai:gpt-4.1", "anthropic:claude-4-sonnet"],
  "temperature": 0.2,
  "sourceFiles": ["*.ts", "!tests/**"]
}
```

*Router returns `modelUsed` and `costUSD` for transparency.*

---

## 9 | Business Model & Pricing

| Plan               | Price           | Target        | Key Limits                     |
| ------------------ | --------------- | ------------- | ------------------------------ |
| **Developer**      | \$39/mo         | Indies        | 15 k cloud tokens/day          |
| **Team**           | \$99/mo         | 3â€‘50 dev orgs | Unlimited cloud; XR collab     |
| **Enterprise**     | \$299/mo        | 50â€‘5 000 devs | Private VPC, audit, SSO        |
| **G3D Enterprise** | from \$100 k/yr | Fortuneâ€‘500   | Onâ€‘prem cluster, custom models |

Marketplace revenue split 80/20 (creator/aura).

---

## 10 | KPIs

### Core Performance Metrics
* **< 60 ms** local inference latency (95p)
* **< 2 s** cloud task latency (95p)
* **95 %+** HumanEval pass@1 on cloud tier
* **92.9 %+** HumanEval pass@1 on local Qwen3-Coder

### Context & Memory Performance ðŸ†•
* **â‰¤ 50 ms** context retrieval latency (95p)
* **â‰¤ 300 MB** extra RAM footprint (100k LOC)
* **â‰¤ 1 %** disk overhead vs repo size
* **â‰¥ 70 %** context hit rate
* **+15 pp** LLM suggestion validity improvement

### Business Metrics
* **4.9 / 5** developer NPS
* **< 2 %** monthly churn
* **> 92 %** gross margin
* **99.99 %** uptime

---

## 11 | Risks & Mitigations

| Risk               | Impact | Mitigation                                            |
| ------------------ | ------ | ----------------------------------------------------- |
| Model supply drift | Medium | Maintain model mesh; hotâ€‘swap OSS weights             |
| GPU cost spikes    | High   | Localâ€‘first default; MoE sparsity; batch distillation |
| Data breach        | High   | Clientâ€‘side encryption; zeroâ€‘telemetry mode           |
| UX complexity      | Medium | Progressiveâ€‘reveal UI; user studies & A/B tests       |
| OSS licence clash  | Low    | Vet thirdâ€‘party models; SPDX tracking                 |

---

## 12 | Next Steps

### Immediate Actions (Phase 1 Completion)
1. **Complete Model Infrastructure** - Finish Phase 1.5 model download system for 7 local models.
2. **Implement Context Engine** - Deploy Phase 1.6 Dynamic Context Persistence (4 core services).
3. **Model Integration Testing** - Benchmark all 7 local models + 2 cloud APIs on target hardware.
4. **Storage Optimization** - Implement intelligent model management for 110GB+ downloads.

### Foundation Setup (Phases 1.6-2.0)
5. **Context System GA** - Deploy Phase 2.0 Context Engine with MemoryPanel and MemorySettings.
6. **Performance Validation** - Achieve â‰¤50ms context retrieval and â‰¤300MB RAM targets.
7. **Model Router Optimization** - Fine-tune intelligent routing across 9 model endpoints.

### Strategic Initiatives 
8. **Design Partner Program** - Secure 2 Fortune-500 enterprise pilots for context system.
9. **UX Research** - Run workshops for 3D visual metaphors & 7-model selection UX.
10. **Marketplace Foundation** - Establish plugin SDK and early extension author program.

---

*Prepared by Product Strategy â€“ aura, July 2025*